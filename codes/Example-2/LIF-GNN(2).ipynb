{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d4480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import sparse\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class GraphDataSet(Dataset):\n",
    "    def __init__(self, num_data, graph_seq):\n",
    "        max_node = 300\n",
    "        num_features = 5\n",
    "        for i in range(num_data):\n",
    "            ind = graph_seq[i]\n",
    "            # load files\n",
    "            file_paths = ['data/structure-{}/neighbor.txt'.format(ind), 'data/structure-{}/feature.txt'.format(ind),\n",
    "                          'data/structure-{}/property.txt'.format(ind)]\n",
    "\n",
    "            graph_elements = [np.loadtxt(file_paths[0]), np.loadtxt(file_paths[1]), np.loadtxt(file_paths[2])]\n",
    "\n",
    "            # feature data manipulation\n",
    "            graph_elements[1] = manipulate_feature(graph_elements[1], max_node, num_features)\n",
    "\n",
    "            # normalize the adjacency matrix\n",
    "            graph_elements[0] = normalize_adj(graph_elements[0], max_node)\n",
    "\n",
    "            # delete data points with negative properties\n",
    "            graph_elements[2] = graph_elements[2][graph_elements[2].min(axis=1) >= 0, :]\n",
    "            # get the dimension of proprty\n",
    "            num_properties, width = np.shape(graph_elements[2])\n",
    "            # independent variable t, the external field\n",
    "            t = np.delete(graph_elements[2], 1, axis=1)\n",
    "            # label, the magnetostriction\n",
    "            label = np.delete(graph_elements[2], 0, axis=1)\n",
    "\n",
    "            # change it to the several data points\n",
    "            multiple_neighbor, multiple_feature = [graph_elements[0] for x in range(num_properties)], \\\n",
    "                                                  [graph_elements[1] for x in range(num_properties)]\n",
    "\n",
    "                # concatenating the matrices\n",
    "            if i == 0:\n",
    "                adjacency_matrix, node_attr_matrix, t_matrix, label_matrix = multiple_neighbor, multiple_feature, t, label\n",
    "            else:\n",
    "                adjacency_matrix, node_attr_matrix, t_matrix, label_matrix = np.concatenate((adjacency_matrix, multiple_neighbor)), \\\n",
    "                                                                             np.concatenate((node_attr_matrix, multiple_feature)), \\\n",
    "                                                                             np.concatenate((t_matrix, t)),\\\n",
    "                                                                             np.concatenate((label_matrix, label))\n",
    "\n",
    "        # normalize the independent variable t matrix\n",
    "        t_matrix, label_matrix = normalize_t_label(t_matrix, label_matrix)\n",
    "\n",
    "        self.adjacency_matrix = np.array(adjacency_matrix)\n",
    "        self.node_attr_matrix = np.array(node_attr_matrix)\n",
    "        self.t_matrix = np.array(t_matrix)\n",
    "        self.label_matrix = np.array(label_matrix)\n",
    "\n",
    "        print('--------------------')\n",
    "        print('Training Data:')\n",
    "        print('adjacency matrix:\\t', self.adjacency_matrix.shape)\n",
    "        print('node attribute matrix:\\t', self.node_attr_matrix.shape)\n",
    "        print('t matrix:\\t\\t', self.t_matrix.shape)\n",
    "        print('label name:\\t\\t', self.label_matrix.shape)\n",
    "        print('--------------------')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.adjacency_matrix)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        adjacency_matrix = self.adjacency_matrix[idx].todense()\n",
    "        node_attr_matrix = self.node_attr_matrix[idx].todense()\n",
    "        t_matrix = self.t_matrix[idx]\n",
    "        label_matrix = self.label_matrix[idx]\n",
    "\n",
    "        adjacency_matrix = torch.from_numpy(adjacency_matrix)\n",
    "        node_attr_matrix = torch.from_numpy(node_attr_matrix)\n",
    "        t_matrix = torch.from_numpy(t_matrix)\n",
    "        label_matrix = torch.from_numpy(label_matrix)\n",
    "        return adjacency_matrix, node_attr_matrix, t_matrix, label_matrix\n",
    "\n",
    "def normalize_adj(neighbor, max_node):\n",
    "    np.fill_diagonal(neighbor, 1)  # add the identity matrix\n",
    "    D = np.sum(neighbor, axis=0)  # calculate the diagnoal element of D\n",
    "    D_inv = np.diag(np.power(D, -0.5))  # construct D\n",
    "    neighbor = np.matmul(D_inv, np.matmul(neighbor, D_inv))  # symmetric normalization of adjacency matrix\n",
    "\n",
    "    # match dimension to the max dimension for neighbors\n",
    "    result = np.zeros((max_node, max_node))\n",
    "    result[:neighbor.shape[0], :neighbor.shape[1]] = neighbor\n",
    "    neighbor = result\n",
    "\n",
    "    # convert the feature matrix to sparse matrix\n",
    "    neighbor = sparse.csr_matrix(neighbor)\n",
    "\n",
    "    return neighbor\n",
    "\n",
    "def manipulate_feature(feature, max_node, features):\n",
    "    feature = np.delete(feature, 0, axis=1)  # remove the first column (Grain ID)\n",
    "    feature[:, [3]] = (feature[:, [3]] - np.mean(feature[:, [3]])) / np.std(\n",
    "        feature[:, [3]])  # normalize grain size\n",
    "    feature[:, [4]] = (feature[:, [4]] - np.mean(feature[:, [4]])) / np.std(\n",
    "        feature[:, [4]])  # normalize number of neighbors\n",
    "\n",
    "    # match dimension to the max dimension for features\n",
    "    result = np.zeros((max_node, features))\n",
    "    result[:feature.shape[0], :feature.shape[1]] = feature\n",
    "    feature = result\n",
    "\n",
    "    # convert the feature matrix to sparse matrix\n",
    "    feature = sparse.csr_matrix(feature)\n",
    "\n",
    "    return feature\n",
    "\n",
    "def normalize_t_label(t_matrix, label_matrix):\n",
    "    t_matrix = t_matrix / 10000\n",
    "    label_mean = np.mean(label_matrix)\n",
    "    label_std = np.std(label_matrix)\n",
    "    label_matrix = (label_matrix - label_mean) / label_std\n",
    "\n",
    "    # save the mean and standard deviation of label\n",
    "    norm = np.array([label_mean, label_std])\n",
    "    np.savez_compressed('norm.npz', norm=norm)\n",
    "\n",
    "    return t_matrix, label_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01919678-33fb-43a9-a11c-bf67d556b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "from util import *\n",
    "# from data import *\n",
    "import sys\n",
    "import snntorch as snn\n",
    "\n",
    "class Message_Passing(nn.Module):\n",
    "    def forward(self, x, adjacency_matrix):\n",
    "        neighbor_nodes = torch.bmm(adjacency_matrix, x)\n",
    "        logging.debug('neighbor message\\t', neighbor_nodes.size())\n",
    "        logging.debug('x shape\\t', x.size())\n",
    "        return neighbor_nodes\n",
    "\n",
    "class GraphModel(nn.Module):\n",
    "    def __init__(self, max_node_num, atom_attr_dim, latent_dim1, latent_dim2):\n",
    "        super(GraphModel, self).__init__()\n",
    "\n",
    "        self.max_node_num = max_node_num\n",
    "        self.atom_attr_dim = atom_attr_dim\n",
    "        self.latent_dim1 = latent_dim1\n",
    "        self.latent_dim2 = latent_dim2\n",
    "\n",
    "        self.message_passing_0 = Message_Passing()\n",
    "        self.dense_0 = nn.Linear(self.atom_attr_dim, self.latent_dim2)\n",
    "        self.activation_0 = nn.Sigmoid()\n",
    "        self.message_passing_1 = Message_Passing()\n",
    "        self.dense_1 = nn.Linear(self.latent_dim2, self.latent_dim1)\n",
    "        self.activation_1 = nn.Sigmoid()\n",
    "\n",
    "        self.fc1 = nn.Linear(self.max_node_num * self.latent_dim1 + 1, 1024)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 1)\n",
    "        \n",
    "        # defining the spiking neurons\n",
    "        \n",
    "        beta_1 = torch.rand(1024)\n",
    "        thr_1 = torch.rand(1024)\n",
    "        self.lif_1 = snn.Leaky(beta=beta_1, learn_beta=True, threshold=thr_1, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_2 = torch.rand(128)\n",
    "        thr_2 = torch.rand(128)\n",
    "        self.lif_2 = snn.Leaky(beta=beta_2, learn_beta=True, threshold=thr_2, learn_threshold=True, reset_mechanism='zero')\n",
    "        \n",
    "        beta_3 = torch.rand(300,40)\n",
    "        thr_3 = torch.rand(300,40)\n",
    "        self.lif_3 = snn.Leaky(beta=beta_3, learn_beta=True, threshold=thr_3, learn_threshold=True, reset_mechanism='zero')\n",
    "        \n",
    "        beta_4 = torch.rand(300,3)\n",
    "        thr_4 = torch.rand(300,3)\n",
    "        self.lif_4 = snn.Leaky(beta=beta_4, learn_beta=True, threshold=thr_4, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        return\n",
    "\n",
    "    def forward(self, node_attr_matrix, adjacency_matrix, t_matrix):\n",
    "        node_attr_matrix = node_attr_matrix.float()\n",
    "        adjacency_matrix = adjacency_matrix.float()\n",
    "        x = node_attr_matrix\n",
    "        logging.debug('shape\\t', x.size())\n",
    " \n",
    "        #defining memory for spiking neurons\n",
    "        mem_1 = self.lif_1.init_leaky()\n",
    "        mem_2 = self.lif_2.init_leaky()\n",
    "        mem_3 = self.lif_3.init_leaky()\n",
    "        mem_4 = self.lif_4.init_leaky()\n",
    "        \n",
    "        \n",
    "        s1_sum = torch.zeros([1]).to('cuda:0')\n",
    "        s2_sum = torch.zeros([1]).to('cuda:0')\n",
    "        s3_sum = torch.zeros([1]).to('cuda:0')\n",
    "        s4_sum = torch.zeros([1]).to('cuda:0')\n",
    "        \n",
    "        \n",
    "        # Message Passing Layer 1\n",
    "        x = self.message_passing_0(x, adjacency_matrix)\n",
    "        x = self.dense_0(x)\n",
    "#         x = self.activation_0(x)\n",
    "\n",
    "        spk_in3, mem_3 = self.lif_3(x, mem_3)\n",
    "        x = spk_in3\n",
    "        s3_sum[0] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "\n",
    "        # Message Passing Layer 2\n",
    "        x = self.message_passing_1(x, adjacency_matrix)\n",
    "        x = self.dense_1(x)\n",
    "#         x = self.activation_1(x)\n",
    "\n",
    "        spk_in4, mem_4 = self.lif_4(x, mem_4)\n",
    "        x = spk_in4\n",
    "        s4_sum[0] += torch.sum(spk_in4)/spk_in4.numel()\n",
    "\n",
    "        # Before flatten, the size should be [Batch size, max_node_num, latent_dim]\n",
    "        logging.debug('size of x after GNN\\t', x.size())\n",
    "        # After flatten is the graph representation\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        logging.debug('size of x after GNN\\t', x.size())\n",
    "\n",
    "        # Concatenate [x, t]\n",
    "        x = torch.cat((x, t_matrix), 1)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "\n",
    "\n",
    "        spk_in1, mem_1 = self.lif_1(x, mem_1)\n",
    "        x = spk_in1\n",
    "        s1_sum[0] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        spk_in2, mem_2 = self.lif_2(x, mem_2)\n",
    "        x = spk_in2\n",
    "        s2_sum[0] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x,s1_sum,s2_sum\n",
    "    \n",
    "\n",
    "def train(model, train_data_loader, validation_data_loader, epochs, checkpoint_dir, optimizer, criterion, validation_index, folder_name):\n",
    "    print()\n",
    "    print(\"*** Training started! ***\")\n",
    "    print()\n",
    "    \n",
    "    filename='{}/learning_Output_{}.txt'.format(folder_name, validation_index)\n",
    "    output=open(filename, \"w\")\n",
    "    print('Epoch Training_time Training_MSE Validation_MSE',file=output, flush = True)  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # print(\"hi\")\n",
    "        model.train()\n",
    "        total_macro_loss = []\n",
    "        total_mse_loss = []\n",
    "\n",
    "        train_start_time = time.time()\n",
    "\n",
    "        for batch_id, (adjacency_matrix, node_attr_matrix, t_matrix, label_matrix) in enumerate(train_data_loader):\n",
    "            # print(\"yo\")\n",
    "            # print(x)\n",
    "            adjacency_matrix = tensor_to_variable(adjacency_matrix)\n",
    "            node_attr_matrix = tensor_to_variable(node_attr_matrix)\n",
    "            t_matrix = tensor_to_variable(t_matrix)\n",
    "            label_matrix = tensor_to_variable(label_matrix)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred,s1_t,s2_t = model(adjacency_matrix=adjacency_matrix, node_attr_matrix=node_attr_matrix, t_matrix=t_matrix)\n",
    "            loss = criterion(y_pred, label_matrix)\n",
    "            total_macro_loss.append(macro_avg_err(y_pred, label_matrix).item())\n",
    "            total_mse_loss.append((loss.item()))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_end_time = time.time()\n",
    "        _, training_loss_epoch, _, _ = test(model, train_data_loader, 'Training', False, criterion, validation_index, folder_name) \n",
    "        _, validation_loss_epoch, _, _ = test(model, validation_dataloader, 'Validation', False, criterion, validation_index, folder_name)\n",
    "        print('%d %.3f %e %e' % (epoch, train_end_time-train_start_time, training_loss_epoch, validation_loss_epoch), file=output,flush=True )\n",
    "\n",
    "def test(model, data_loader, test_val_tr, printcond, criterion, running_index, folder_name):\n",
    "    model.eval()\n",
    "    if data_loader is None:\n",
    "        return None, None\n",
    "\n",
    "    y_label_list, y_pred_list, total_loss = [], [], 0\n",
    "\n",
    "    for batch_id, (adjacency_matrix, node_attr_matrix, t_matrix, label_matrix) in enumerate(data_loader):\n",
    "        adjacency_matrix = tensor_to_variable(adjacency_matrix)\n",
    "        node_attr_matrix = tensor_to_variable(node_attr_matrix)\n",
    "        t_matrix = tensor_to_variable(t_matrix)\n",
    "        label_matrix = tensor_to_variable(label_matrix)\n",
    "\n",
    "        y_pred,s1_test,s2_test = model(adjacency_matrix=adjacency_matrix, node_attr_matrix=node_attr_matrix, t_matrix=t_matrix)\n",
    "\n",
    "        y_label_list.extend(variable_to_numpy(label_matrix))\n",
    "        y_pred_list.extend(variable_to_numpy(y_pred))\n",
    "\n",
    "    norm = np.load('norm.npz', allow_pickle=True)['norm']\n",
    "    label_mean, label_std = norm[0], norm[1]\n",
    "\n",
    "    y_label_list = np.array(y_label_list) * label_std + label_mean\n",
    "    y_pred_list = np.array(y_pred_list) * label_std + label_mean\n",
    "\n",
    "    total_loss = macro_avg_err(y_pred_list, y_label_list)\n",
    "    total_mse = criterion(torch.from_numpy(y_pred_list), torch.from_numpy(y_label_list)).item()\n",
    "\n",
    "    length, w = np.shape(y_label_list)\n",
    "    if printcond:\n",
    "        filename = '{}/{}_Output_{}.txt'.format(folder_name, test_val_tr, running_index)\n",
    "        output = open(filename, 'w')\n",
    "        #print()\n",
    "        print('{} Set Predictions: '.format(test_val_tr), file = output, flush = True)\n",
    "        print('True_value Predicted_value', file=output, flush = True)\n",
    "        for i in range(0, length):\n",
    "            print('%f, %f' % (y_label_list[i], y_pred_list[i]),file=output,flush = True)\n",
    "\n",
    "    return total_loss, total_mse, s1_test, s2_test\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    if 'ipykernel' in sys.modules:\n",
    "        args = ['--max_node_num','300','--atom_attr_dim','5','--num_graphs','492','--batch_size','32','--min_learning_rate','0','--seed','123','--checkpoint','checkpoints/','--validation_index','0','--testing_index','1','--folds','10','--idx_path','data/indices_and_graphseq.npz','--folder_name','output_test_spk1/','--num_data','492','--hyper','0']\n",
    "        \n",
    "    else:\n",
    "        args = sys.argv[1:]\n",
    "        \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--max_node_num', type=int, default=300)\n",
    "    parser.add_argument('--atom_attr_dim', type=int, default=5)\n",
    "    parser.add_argument('--num_graphs', type=int, default=492)\n",
    "    parser.add_argument('--batch_size', type=int, default=32)\n",
    "    parser.add_argument('--min_learning_rate', type=float, default=0)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    parser.add_argument('--checkpoint', type=str, default='checkpoints/')\n",
    "    parser.add_argument('--validation_index', type=int, default=0)\n",
    "    parser.add_argument('--testing_index', type=int, default=1)\n",
    "    parser.add_argument('--folds', type=int, default=10)\n",
    "    parser.add_argument('--idx_path', type=str, default='data/indices_and_graphseq.npz')\n",
    "    parser.add_argument('--folder_name', type=str, default='output_test_spk1/')\n",
    "    parser.add_argument('--num_data', type=int, default=492)\n",
    "    parser.add_argument('--hyper',type=int,default=0)\n",
    "\n",
    "    given_args = parser.parse_args(args)\n",
    "    max_node_num = given_args.max_node_num\n",
    "    atom_attr_dim = given_args.atom_attr_dim\n",
    "    num_graphs = given_args.num_graphs\n",
    "    checkpoint_dir = given_args.checkpoint\n",
    "    validation_index = given_args.validation_index\n",
    "    testing_index = given_args.testing_index\n",
    "    idx_path = given_args.idx_path\n",
    "    folds = given_args.folds\n",
    "    batch_size = given_args.batch_size\n",
    "    min_learning_rate = given_args.min_learning_rate\n",
    "    seed = given_args.seed\n",
    "    checkpoint_dir = given_args.checkpoint\n",
    "    folds = given_args.folds\n",
    "    idx_path = given_args.idx_path\n",
    "    folder_name = given_args.folder_name\n",
    "    num_data = given_args.num_data\n",
    "    hyper=given_args.hyper\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    \n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    os.environ['PYTHONHASHargs.seed'] = str(given_args.seed)\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "    np.random.seed(given_args.seed)\n",
    "    torch.manual_seed(given_args.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(given_args.seed)\n",
    "        torch.cuda.manual_seed_all(given_args.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    filename='hyper1/hyper/'+str(hyper)+'.json'\n",
    "    with open(filename,'r') as h:\n",
    "        hyperset=json.load(h)\n",
    "        \n",
    "    latent_dim1=hyperset['latent_dim1']\n",
    "    latent_dim2=hyperset['latent_dim2']\n",
    "    epochs=hyperset['epoch']\n",
    "    learning_rate=hyperset['lr']\n",
    "    in_optim=hyperset['optim']\n",
    "\n",
    "    # Define the model\n",
    "    model = GraphModel(max_node_num, atom_attr_dim, latent_dim1, latent_dim2)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "        \n",
    "    if in_optim==\"Adam\":\n",
    "        optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    elif in_optim==\"RMSprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(),lr=learning_rate)\n",
    "    elif in_optim==\"SGD\":\n",
    "        optimizer = optim.SGD(model.parameters(),lr=learning_rate)\n",
    "        \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        '''Seeding for DataLoaders'''\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "\n",
    "    # Load data\n",
    "    train_loader = torch.load(\"Example-2/train_loader.pt\",weights_only=False)\n",
    "    val_loader = torch.load(\"Example-2/val_loader.pt\",weights_only=False)\n",
    "    test_loader = torch.load(\"Example-2/test_loader.pt\",weights_only=False)\n",
    "    \n",
    "\n",
    "    # train the model\n",
    "    train_start_time = time.time()\n",
    "    train(model, train_dataloader, validation_dataloader,epochs, checkpoint_dir, optimizer, criterion, validation_index,folder_name)\n",
    "    train_end_time = time.time()\n",
    "\n",
    "    torch.save(model, '{}/checkpoint.pth'.format(checkpoint_dir))    \n",
    "    \n",
    "    # predictions on the entire training and test datasets\n",
    "    train_rel, train_mse, sa_t, sb_t= test(model, train_dataloader, 'Training', True, criterion, validation_index, folder_name)\n",
    "    validation_rel, validation_mse, se_t, sf_t=test(model, validation_dataloader, 'Validation', True, criterion, validation_index, folder_name)\n",
    "    test_start_time = time.time()\n",
    "    test_rel, test_mse, s1_test, s2_test= test(model, test_dataloader, 'Test', True, criterion, testing_index, folder_name)\n",
    "    test_end_time = time.time()\n",
    "\n",
    "    \n",
    "    print('--------------------')\n",
    "    print(\"validation_index : {}\".format(validation_index))\n",
    "    print(\"testing_index : {}\".format(testing_index))\n",
    "    print(\"training_time : {}\".format(train_end_time-train_start_time))\n",
    "    print(\"testing_time : {}\".format(test_end_time-test_start_time))\n",
    "    print(\"Train Relative Error: {:.3f}%\".format(100 * train_rel))\n",
    "    print(\"Validation Relative Error: {:.3f}%\".format(100 * validation_rel))\n",
    "    print(\"Test Relative Error: {:.3f}%\".format(100 * test_rel))\n",
    "    print(\"Train MSE : {}\".format(train_mse))\n",
    "    print(\"Validation MSE : {}\".format(validation_mse))\n",
    "    print(\"Test MSE: {}\".format(test_mse))\n",
    "    print(\"Spking Activity for s1: \",s1_test)\n",
    "    print(\"Spking Activity for s2: \",s2_test)\n",
    "    # print(\"Spking Activity for s3: \",s3_test)\n",
    "    # print(\"Spking Activity for s4: \",s4_test)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e1dc5",
   "metadata": {},
   "source": [
    "Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cfee12-da51-4fba-a030-b66e7932f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the true and predicted values from the output files\n",
    "def load_predictions(output_file):\n",
    "    true_values = []\n",
    "    predicted_values = []\n",
    "    with open(output_file, 'r') as f:\n",
    "        next(f)  # Skip the header\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            true_value, predicted_value = map(float, line.strip().split(','))\n",
    "            true_values.append(true_value)\n",
    "            predicted_values.append(predicted_value)\n",
    "    return true_values, predicted_values\n",
    "\n",
    "# Load the output file for the test set\n",
    "test_output_file = 'outputs/Test_Output_1.txt'  # Adjust the file path as needed\n",
    "\n",
    "# Load true and predicted values for the test set\n",
    "test_true, test_pred = load_predictions(test_output_file)\n",
    "\n",
    "# Scale the predicted values by 10^6 to convert them to ppm\n",
    "test_pred = np.array(test_pred) * 10**6\n",
    "test_true = np.array(test_true) * 10**6  # Scale true values similarly if needed\n",
    "\n",
    "# Plot true vs predicted values for the test dataset\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Scatter plot with outlined circles in a subtle color\n",
    "plt.scatter(test_true, test_pred, facecolors='none', edgecolors='blue', label='Test')\n",
    "\n",
    "# Set axis titles with the required math symbols and formatting and larger font size\n",
    "plt.xlabel(r'$\\mathrm{True\\ \\lambda^{eff}_{xx}\\ (ppm)}$', fontsize=16)  # Adjust fontsize here\n",
    "plt.ylabel(r'$\\mathrm{Predicted\\ \\lambda^{eff}_{xx}\\ (ppm)}$', fontsize=16)  # Adjust fontsize here\n",
    "# plt.title('VSNN-SLF(a)')\n",
    "plt.legend()\n",
    "\n",
    "# Fit a least squares line\n",
    "coeffs = np.polyfit(test_true, test_pred, 1)\n",
    "least_squares_line = np.polyval(coeffs, test_true)\n",
    "\n",
    "# Plot the least squares line\n",
    "plt.plot(test_true, least_squares_line, color='black', linestyle='--', label='Least Squares Line')\n",
    "plt.legend()\n",
    "\n",
    "# Set x and y ticks to be 0, 800, 1600\n",
    "tick_values = [0, 800, 1600]\n",
    "plt.xticks(tick_values, labels=[f'{x} ppm' for x in tick_values])\n",
    "plt.yticks(tick_values, labels=[f'{x} ppm' for x in tick_values])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
