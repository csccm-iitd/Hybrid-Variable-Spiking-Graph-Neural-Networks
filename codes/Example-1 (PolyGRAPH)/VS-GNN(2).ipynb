{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "028bd818-0b94-42d3-88b9-a89787b67565",
   "metadata": {
    "collapsed": true,
    "id": "028bd818-0b94-42d3-88b9-a89787b67565",
    "outputId": "35b9d4a1-6cbe-4dc8-e9fb-56e6b06deec0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 677, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 471, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 460, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 367, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 662, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 360, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2863, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3309, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3369, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_34836\\3177249315.py\", line 10, in <cell line: 10>\n",
      "    from sklearn.metrics import mean_squared_error as MSE\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 17, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 21, in <module>\n",
      "    from scipy.sparse import issparse\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\__init__.py\", line 228, in <module>\n",
      "    from .csr import *\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\csr.py\", line 10, in <module>\n",
      "    from ._sparsetools import (csr_tocsc, csr_tobsr, csr_count_blocks,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error \u001b[38;5;28;01mas\u001b[39;00m MSE\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m r2_score \u001b[38;5;28;01mas\u001b[39;00m R2\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Linear, SAGEConv, global_mean_pool\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     20\u001b[0m     _safe_tags,\n\u001b[0;32m     21\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_X_y\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmurmurhash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m murmurhash3_32\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclass_weight\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\__init__.py:228\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_warnings\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcsc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\csr.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spmatrix\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_sparsetools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (csr_tocsc, csr_tobsr, csr_count_blocks,\n\u001b[0;32m     11\u001b[0m                            get_csr_submatrix)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msputils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m upcast, get_index_dtype\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _cs_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from torch_geometric.nn import Linear, SAGEConv, global_mean_pool\n",
    "import time\n",
    "import snntorch as snn\n",
    "import pickle\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    '''\n",
    "    Set random seeds for reproducability\n",
    "    '''\n",
    "    if not seed:\n",
    "        seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.utils import spmm\n",
    "from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n",
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "class SAGEConv_new(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        aggr: str = \"mean\",\n",
    "        normalize: bool = False,\n",
    "        root_weight: bool = True,\n",
    "        project: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__(aggr)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "        self.project = project\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        if self.project:\n",
    "            self.lin = Linear(in_channels[0], in_channels[0], bias=True)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.project:\n",
    "            self.lin.reset_parameters()\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward_without_activation(\n",
    "        self, x: Union[torch.Tensor, OptPairTensor], edge_index: Adj, size: Size = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Performs all operations before activation.\"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = (x, x)\n",
    "\n",
    "        if self.project and hasattr(self, \"lin\"):\n",
    "            x = (self.lin(x[0]), x[1])  # No activation here\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out = out + self.lin_r(x_r)\n",
    "\n",
    "        return out  # Output before activation\n",
    "\n",
    "    def apply_activation(self, x: torch.Tensor, activation_fn=torch.relu) -> torch.Tensor:\n",
    "        \"\"\"Applies the activation function separately.\"\"\"\n",
    "        return activation_fn(x)\n",
    "\n",
    "    def normalize_output(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies normalization separately if needed.\"\"\"\n",
    "        if self.normalize:\n",
    "            return F.normalize(x, p=2.0, dim=-1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: Adj, activation_fn=torch.relu):\n",
    "        \"\"\"Complete forward pass with separate activation.\"\"\"\n",
    "        out = self.forward_without_activation(x, edge_index)\n",
    "        out = self.apply_activation(out, activation_fn)\n",
    "        out = self.normalize_output(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: torch.Tensor) -> torch.Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: Adj, x: OptPairTensor) -> torch.Tensor:\n",
    "        if isinstance(adj_t, SparseTensor):\n",
    "            adj_t = adj_t.set_value(None, layout=None)\n",
    "        return spmm(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "class GNN(torch.nn.Module):\n",
    "    '''\n",
    "    Graph Neural Network\n",
    "    '''\n",
    "    def __init__(self, N_fl1, N_mpl, N_fl2, N_fl3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.pre = Linear(5, N_fl1)\n",
    "        self.conv1 = SAGEConv_new(N_fl1, N_mpl, normalize=True)\n",
    "        self.conv2 = SAGEConv_new(N_mpl, N_mpl, normalize=True)\n",
    "        self.post1 = Linear(N_mpl, N_fl2)\n",
    "        self.post2 = Linear(N_fl2, N_fl3)\n",
    "        self.out = Linear(N_fl3, 1)\n",
    "\n",
    "        # Spiking Neurons\n",
    "        # Neuron 1\n",
    "        beta_1 = torch.rand(32)\n",
    "        thr_1 = torch.rand(32)*0.001\n",
    "        self.lif_1 = snn.Leaky(beta = beta_1, learn_beta = True, threshold = thr_1, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_2 = torch.rand(64)\n",
    "        thr_2 = torch.rand(64)*0.001\n",
    "        self.lif_2 = snn.Leaky(beta = beta_2, learn_beta = True, threshold = thr_2, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_2a = torch.rand(64)\n",
    "        thr_2a = torch.rand(64)*0.001\n",
    "        self.lif_2a = snn.Leaky(beta = beta_2a, learn_beta = True, threshold = thr_2a, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_3 = torch.rand(64)\n",
    "        thr_3 = torch.rand(64)*0.001\n",
    "        self.lif_3 = snn.Leaky(beta = beta_3, learn_beta = True, threshold = thr_3, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_3a = torch.rand(64)\n",
    "        thr_3a = torch.rand(64)*0.001\n",
    "        self.lif_3a = snn.Leaky(beta = beta_3a, learn_beta = True, threshold = thr_3a, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        mem_1 = self.lif_1.init_leaky()\n",
    "        mem_2 = self.lif_2.init_leaky()\n",
    "        mem_2a = self.lif_2a.init_leaky()\n",
    "        mem_3 = self.lif_3.init_leaky()\n",
    "        mem_3a = self.lif_3a.init_leaky()\n",
    "\n",
    "        s1_sum = torch.zeros([1]).to(device)\n",
    "        s2_sum = torch.zeros([1]).to(device)\n",
    "        s2a_sum = torch.zeros([1]).to(device)\n",
    "        s3_sum = torch.zeros([1]).to(device)\n",
    "        s3a_sum = torch.zeros([1]).to(device)\n",
    "\n",
    "        # Pre Processing Linear Layer\n",
    "        # Replacing ReLU with spiking\n",
    "        x1 = self.pre(x)\n",
    "        spk_in1, mem_1 = self.lif_1(x1, mem_1)\n",
    "        x = spk_in1*x1\n",
    "        s1_sum[0] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "\n",
    "        # 1. Obtain node embeddings\n",
    "        # Replacing ReLU with spiking\n",
    "        x2a = self.conv1.forward_without_activation(x, edge_index)\n",
    "        spk_in2a, mem_2a = self.lif_2a(x2a, mem_2a)\n",
    "        x = spk_in2a*x2a\n",
    "        s2a_sum[0] += torch.sum(spk_in2a)/spk_in2a.numel()\n",
    "        \n",
    "        x3a = self.conv2.forward_without_activation(x, edge_index)\n",
    "        spk_in3a, mem_3a = self.lif_3a(x3a, mem_3a)\n",
    "        x = spk_in3a*x3a\n",
    "        s3a_sum[0] += torch.sum(spk_in3a)/spk_in3a.numel()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x = F.relu(self.post1(x))\n",
    "\n",
    "        x = F.relu(self.post2(x))\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x, s1_sum, s2a_sum, s3a_sum\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    '''\n",
    "    Initialize model\n",
    "    '''\n",
    "    seed_all(seed)\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GNN(N_fl1, N_mpl, N_fl2, N_fl3).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=l_rate, weight_decay=w_decay)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, n_epoch, prop, config, fold):\n",
    "    '''\n",
    "    Train GNN\n",
    "    '''\n",
    "    filename = f'{output_dir}/eval-{eval}_config-{config}_fold-{fold}_loss_history.txt'\n",
    "    output = open(filename, \"w\")\n",
    "\n",
    "    print('Epoch Training_MSE Validation_MSE', file=output, flush=True)\n",
    "\n",
    "    seed_all(seed)\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        # Train batches\n",
    "        for train_batch in train_loader:\n",
    "            train_batch = train_batch.to(device)\n",
    "            train_pred, s1_t, s2_t, s3_t = model(train_batch)\n",
    "            train_true = getattr(train_batch, prop)\n",
    "            train_loss = F.mse_loss(train_pred, train_true)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Evaluate\n",
    "        val_pred, val_true, s1_test, s2_test, s3_test = test(model, val_loader, prop)\n",
    "        val_loss = F.mse_loss(val_pred, val_true)\n",
    "        print(f'{epoch:d}, {train_loss:e}, {val_loss:e}', file=output, flush=True)\n",
    "    return\n",
    "\n",
    "\n",
    "def test(model, data_loader, prop):\n",
    "    '''\n",
    "    Test GNN\n",
    "    '''\n",
    "    seed_all(seed)\n",
    "    model.eval()\n",
    "    data = next(iter(data_loader)).to(device)\n",
    "    pred, s1_test, s2_test, s3_test = model(data)\n",
    "    true = getattr(data, prop)\n",
    "    return pred, true, s1_test, s2_test, s3_test\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    eval = 2\n",
    "    prop = 'strength'\n",
    "    config_dir = './'\n",
    "    config = 0\n",
    "    output_dir = './out/'\n",
    "    seed = 42\n",
    "\n",
    "    if not os.path.exists(config_dir):\n",
    "        os.makedirs(config_dir)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    config_name = config_dir + str(config) + '.json'\n",
    "    with open(config_name, 'r') as h:\n",
    "        params = json.load(h)\n",
    "\n",
    "    l_rate = params['l_rate']\n",
    "    w_decay = params['w_decay']\n",
    "    n_epoch = params['n_epoch']\n",
    "    b_size = params['b_size']\n",
    "    N_fl1 = params['N_fl1']\n",
    "    N_mpl = params['N_mpl']\n",
    "    N_fl2 = params['N_fl2']\n",
    "    N_fl3 = params['N_fl3']\n",
    "\n",
    "    # Set seeds for complete reproducability\n",
    "    seed_all(seed)\n",
    "\n",
    "    # Define the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    cases = ['Train (A-G) / Test (A-G)',\n",
    "             'Train (A-G) / Test (H-L)']\n",
    "\n",
    "    print('\\n====== Configuration ======')\n",
    "    print(f'Evaluation #{eval}:\\t\\t{cases[eval-2]}')\n",
    "    print(f'Regression task:\\t{prop}')\n",
    "    print(f'Hyper-parameters :\\t{config}.json')\n",
    "\n",
    "# *************************************************************************** #\n",
    "    print('\\n====== Training / Testing ======')\n",
    "    start = time.time()\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        '''Seeding for DataLoaders'''\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "\n",
    "    # Load data\n",
    "    train_loader = torch.load(\"data/train_dataset.pt\",weights_only=False)\n",
    "    \n",
    "    #### Eval = 2 if you're running Evaluation-1 and Eval = 3 if you're running Evaluation-2\n",
    "    if eval==2:\n",
    "        test_loader = torch.load(\"data/test_dataset_2.pt\",weights_only=False)\n",
    "    else:\n",
    "        test_loader = torch.load(\"data/test_dataset_3.pt\",weights_only=False)\n",
    "\n",
    "    # Define model and optimizer\n",
    "    model, optimizer = init_model()\n",
    "    \n",
    "    #### Load the checkpoint file if you directly want results else train\n",
    "#     model.load_state_dict(torch.load(\"out/ALL_LIF_eval-2_config-0_prop-strength_fold-NA_checkpoint_AFTER_REVISION_NO_NORM.pth\",map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "\n",
    "    #Train model\n",
    "    train(model, optimizer, train_loader, test_loader,\n",
    "          n_epoch, prop, config, 'NA')\n",
    "\n",
    "    # Test Model\n",
    "    preds, trues, s1_test, s2_test, s3_test = test(model, test_loader, prop)\n",
    "\n",
    "    # Save model\n",
    "    # torch.save(model.state_dict(), f\"{output_dir}/ALL_LIF_eval-{eval}_config-{config}_prop-{prop}_fold-{'NA'}_checkpoint_AFTER_REVISION_NO_NORM.pth\")\n",
    "\n",
    "    print(f'Processing time: {time.time()-start:.2f} seconds')\n",
    "# *************************************************************************** #\n",
    "    # Report and Visualize predictions\n",
    "    \n",
    "    with open(\"out/stiffness_scaler.pickle\", \"rb\") as f:\n",
    "        stiffness_scaler = pickle.load(f)\n",
    "\n",
    "    with open(\"out/strength_scaler.pickle\", \"rb\") as f:\n",
    "        strength_scaler = pickle.load(f)\n",
    "\n",
    "    with open(\"out/x_scaler.pickle\", \"rb\") as f:\n",
    "        x_scaler = pickle.load(f)\n",
    "    \n",
    "    scaler = {}\n",
    "    scaler['stiffness'] = stiffness_scaler\n",
    "    scaler['strength'] = strength_scaler\n",
    "    scaler['x'] = x_scaler\n",
    "\n",
    "    print('\\n====== RESULTS ======')\n",
    "    preds = scaler[prop].inverse_transform(\n",
    "        preds.detach().detach().cpu().numpy())\n",
    "    trues = scaler[prop].inverse_transform(\n",
    "        trues.detach().detach().cpu().numpy())\n",
    "    meanARE, maxARE = mean_maxARE(preds, trues)\n",
    "\n",
    "    print(\"Spiking activity is :\" )\n",
    "    print(s1_test)\n",
    "    print(s2_test)\n",
    "    print(s3_test)\n",
    "    print(s4_test)\n",
    "    print(s5_test)\n",
    "\n",
    "    print(f'(MeanARE, MaxARE):\\t({meanARE}, {maxARE})')\n",
    "\n",
    "    def plot_results(preds, trues, output_dir, eval, config, prop):\n",
    "\n",
    "        if prop == 'strength':\n",
    "            preds = preds*1000\n",
    "            trues = trues*1000\n",
    "\n",
    "        \n",
    "        '''Plot evaluation results\n",
    "        '''\n",
    "        sns.set(font_scale=1.75)\n",
    "        sns.set_style(\"ticks\")\n",
    "        # fig, ax = plt.subplots(figsize=(8.5, 5.5), dpi=300)\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "        minColor = 0.4\n",
    "        maxColor = 1.00\n",
    "        if prop == 'strength':\n",
    "            cmap = truncate_colormap(plt.get_cmap(\"Greens\"), minColor, maxColor)\n",
    "        else:\n",
    "            cmap = truncate_colormap(plt.get_cmap(\"Blues\"), minColor, maxColor)\n",
    "        col = mcolors.to_hex(cmap(0.5))\n",
    "    \n",
    "        if eval != 2:\n",
    "            x = np.squeeze(trues)\n",
    "            y = np.squeeze(preds)\n",
    "            xy = np.vstack([x, y])\n",
    "            z = gaussian_kde(xy)(xy)\n",
    "            # Sort the points by density, so that the densest points are plotted last\n",
    "            idx = z.argsort()\n",
    "            x, y, z = x[idx], y[idx], z[idx]\n",
    "    \n",
    "            plt.scatter(x,\n",
    "                        y,\n",
    "                        c=z,\n",
    "                        s=20,\n",
    "                        cmap=cmap)\n",
    "        else:\n",
    "            plt.scatter(trues,\n",
    "                        preds,\n",
    "                        s=20,\n",
    "                        ec='k',\n",
    "                        lw=0.5,\n",
    "                        color=col)\n",
    "        if prop == 'strength':\n",
    "            plt.xlabel('True strength (MPa)')\n",
    "            plt.ylabel('Predicted strength (MPa)')\n",
    "            plt.xlim([700, 1220])\n",
    "            plt.ylim([700, 1220])\n",
    "            plt.plot([700, 1220], [700, 1220], '-k', linewidth=2)\n",
    "        else:\n",
    "            plt.xlabel('True modulus (GPa)')\n",
    "            plt.ylabel('Predicted modulus (GPa)')\n",
    "            plt.xlim([110, 152])\n",
    "            plt.ylim([110, 152])\n",
    "            plt.plot([110, 152], [110, 152], '-k', linewidth=2)\n",
    "    \n",
    "        ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n",
    "    \n",
    "        plt.savefig(f'/mnt/sdb1/graphspiking/graph_spiking/PolyGRAPH-main/REVISION_RESULTS/ALL_LIF_eval-{eval}_prop-{prop}_config-{config}.parity.png', dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "    plot_results(preds/1000, trues/1000, output_dir, eval, config, prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1644d0ba",
   "metadata": {},
   "source": [
    "MSE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "907aaa7e-32fb-4a5d-967d-a223a10cf158",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170.39403, 10.246600955724716, (70, 1), (70, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_norm = scaler[prop].transform(\n",
    "        preds)\n",
    "true_norm = scaler[prop].transform(\n",
    "        trues)\n",
    "\n",
    "np.mean((preds-trues)**2), np.mean((pred_norm-true_norm)**2)*1000, pred_norm.shape, true_norm.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
