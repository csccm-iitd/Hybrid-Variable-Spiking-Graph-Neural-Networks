{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bd818-0b94-42d3-88b9-a89787b67565",
   "metadata": {
    "id": "028bd818-0b94-42d3-88b9-a89787b67565",
    "outputId": "35b9d4a1-6cbe-4dc8-e9fb-56e6b06deec0"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score as R2\n",
    "from torch_geometric.nn import Linear, SAGEConv, global_mean_pool\n",
    "import time\n",
    "import snntorch as snn\n",
    "import pickle\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "def seed_all(seed):\n",
    "    '''\n",
    "    Set random seeds for reproducability\n",
    "    '''\n",
    "    if not seed:\n",
    "        seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.dense.linear import Linear\n",
    "from torch_geometric.utils import spmm\n",
    "from torch_geometric.typing import Adj, OptPairTensor, Size, SparseTensor\n",
    "from typing import Union, Tuple, Optional\n",
    "\n",
    "class SAGEConv_new(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int,\n",
    "        aggr: str = \"mean\",\n",
    "        normalize: bool = False,\n",
    "        root_weight: bool = True,\n",
    "        project: bool = False,\n",
    "        bias: bool = True,\n",
    "    ):\n",
    "        super().__init__(aggr)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "        self.root_weight = root_weight\n",
    "        self.project = project\n",
    "\n",
    "        if isinstance(in_channels, int):\n",
    "            in_channels = (in_channels, in_channels)\n",
    "\n",
    "        if self.project:\n",
    "            self.lin = Linear(in_channels[0], in_channels[0], bias=True)\n",
    "\n",
    "        self.lin_l = Linear(in_channels[0], out_channels, bias=bias)\n",
    "        if self.root_weight:\n",
    "            self.lin_r = Linear(in_channels[1], out_channels, bias=False)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        if self.project:\n",
    "            self.lin.reset_parameters()\n",
    "        self.lin_l.reset_parameters()\n",
    "        if self.root_weight:\n",
    "            self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward_without_activation(\n",
    "        self, x: Union[torch.Tensor, OptPairTensor], edge_index: Adj, size: Size = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Performs all operations before activation.\"\"\"\n",
    "        if isinstance(x, torch.Tensor):\n",
    "            x = (x, x)\n",
    "\n",
    "        if self.project and hasattr(self, \"lin\"):\n",
    "            x = (self.lin(x[0]), x[1])  # No activation here\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, size=size)\n",
    "        out = self.lin_l(out)\n",
    "\n",
    "        x_r = x[1]\n",
    "        if self.root_weight and x_r is not None:\n",
    "            out = out + self.lin_r(x_r)\n",
    "\n",
    "        return out  # Output before activation\n",
    "\n",
    "    def apply_activation(self, x: torch.Tensor, activation_fn=torch.relu) -> torch.Tensor:\n",
    "        \"\"\"Applies the activation function separately.\"\"\"\n",
    "        return activation_fn(x)\n",
    "\n",
    "    def normalize_output(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Applies normalization separately if needed.\"\"\"\n",
    "        if self.normalize:\n",
    "            return F.normalize(x, p=2.0, dim=-1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: Adj, activation_fn=torch.relu):\n",
    "        \"\"\"Complete forward pass with separate activation.\"\"\"\n",
    "        out = self.forward_without_activation(x, edge_index)\n",
    "        out = self.apply_activation(out, activation_fn)\n",
    "        out = self.normalize_output(out)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: torch.Tensor) -> torch.Tensor:\n",
    "        return x_j\n",
    "\n",
    "    def message_and_aggregate(self, adj_t: Adj, x: OptPairTensor) -> torch.Tensor:\n",
    "        if isinstance(adj_t, SparseTensor):\n",
    "            adj_t = adj_t.set_value(None, layout=None)\n",
    "        return spmm(adj_t, x[0], reduce=self.aggr)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "class GNN(torch.nn.Module):\n",
    "    '''\n",
    "    Graph Neural Network\n",
    "    '''\n",
    "    def __init__(self, N_fl1, N_mpl, N_fl2, N_fl3):\n",
    "        super(GNN, self).__init__()\n",
    "        self.pre = Linear(5, N_fl1)\n",
    "        self.conv1 = SAGEConv_new(N_fl1, N_mpl, normalize=True)\n",
    "        self.conv2 = SAGEConv_new(N_mpl, N_mpl, normalize=True)\n",
    "        self.post1 = Linear(N_mpl, N_fl2)\n",
    "        self.post2 = Linear(N_fl2, N_fl3)\n",
    "        self.out = Linear(N_fl3, 1)\n",
    "\n",
    "        # Spiking Neurons\n",
    "        # Neuron 1\n",
    "        beta_1 = torch.rand(32)\n",
    "        thr_1 = torch.rand(32)*0.001\n",
    "        self.lif_1 = snn.Leaky(beta = beta_1, learn_beta = True, threshold = thr_1, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_2 = torch.rand(64)\n",
    "        thr_2 = torch.rand(64)*0.001\n",
    "        self.lif_2 = snn.Leaky(beta = beta_2, learn_beta = True, threshold = thr_2, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_2a = torch.rand(64)\n",
    "        thr_2a = torch.rand(64)*0.001\n",
    "        self.lif_2a = snn.Leaky(beta = beta_2a, learn_beta = True, threshold = thr_2a, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_3 = torch.rand(64)\n",
    "        thr_3 = torch.rand(64)*0.001\n",
    "        self.lif_3 = snn.Leaky(beta = beta_3, learn_beta = True, threshold = thr_3, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_3a = torch.rand(64)\n",
    "        thr_3a = torch.rand(64)*0.001\n",
    "        self.lif_3a = snn.Leaky(beta = beta_3a, learn_beta = True, threshold = thr_3a, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_4 = torch.rand(64)\n",
    "        thr_4 = torch.rand(64)*0.001\n",
    "        self.lif_4 = snn.Leaky(beta = beta_4, learn_beta = True, threshold = thr_4, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "        beta_5 = torch.rand(16)\n",
    "        thr_5 = torch.rand(16)*0.001\n",
    "        self.lif_5 = snn.Leaky(beta = beta_5, learn_beta = True, threshold = thr_5, learn_threshold=True, reset_mechanism='zero')\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        mem_1 = self.lif_1.init_leaky()\n",
    "        mem_2 = self.lif_2.init_leaky()\n",
    "        mem_2a = self.lif_2a.init_leaky()\n",
    "        mem_3 = self.lif_3.init_leaky()\n",
    "        mem_3a = self.lif_3a.init_leaky()\n",
    "        mem_4 = self.lif_4.init_leaky()\n",
    "        mem_5 = self.lif_5.init_leaky()\n",
    "\n",
    "        s1_sum = torch.zeros([1]).to(device)\n",
    "        s2_sum = torch.zeros([1]).to(device)\n",
    "        s2a_sum = torch.zeros([1]).to(device)\n",
    "        s3_sum = torch.zeros([1]).to(device)\n",
    "        s3a_sum = torch.zeros([1]).to(device)\n",
    "        s4_sum = torch.zeros([1]).to(device)\n",
    "        s5_sum = torch.zeros([1]).to(device)\n",
    "\n",
    "        # Pre Processing Linear Layer\n",
    "        # Replacing ReLU with spiking\n",
    "        x1 = self.pre(x)\n",
    "        spk_in1, mem_1 = self.lif_1(x1, mem_1)\n",
    "        x = spk_in1*x1\n",
    "        s1_sum[0] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "\n",
    "        # 1. Obtain node embeddings\n",
    "        # Replacing ReLU with spiking\n",
    "        x2a = self.conv1.forward_without_activation(x, edge_index)\n",
    "        spk_in2a, mem_2a = self.lif_2a(x2a, mem_2a)\n",
    "        x = spk_in2a*x2a\n",
    "        s2a_sum[0] += torch.sum(spk_in2a)/spk_in2a.numel()\n",
    "        \n",
    "        x3a = self.conv2.forward_without_activation(x, edge_index)\n",
    "        spk_in3a, mem_3a = self.lif_3a(x3a, mem_3a)\n",
    "        x = spk_in3a*x3a\n",
    "        s3a_sum[0] += torch.sum(spk_in3a)/spk_in3a.numel()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)\n",
    "        \n",
    "        x4 = self.post1(x)\n",
    "        spk_in4, mem_4 = self.lif_4(x4, mem_4)\n",
    "        x = spk_in4*x4a\n",
    "        s4_sum[0] += torch.sum(spk_in4)/spk_in4.numel()\n",
    "\n",
    "\n",
    "        x5 = self.post2(x)\n",
    "        spk_in5, mem_5 = self.lif_5(x5, mem_5)\n",
    "        x = spk_in5*x5\n",
    "        s5_sum[0] += torch.sum(spk_in5)/spk_in5.numel()\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x, s1_sum, s2a_sum, s3a_sum, s4_sum, s5_sum\n",
    "\n",
    "\n",
    "def init_model():\n",
    "    '''\n",
    "    Initialize model\n",
    "    '''\n",
    "    seed_all(seed)\n",
    "    device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "    model = GNN(N_fl1, N_mpl, N_fl2, N_fl3).to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=l_rate, weight_decay=w_decay)\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_loader, val_loader, n_epoch, prop, config, fold):\n",
    "    '''\n",
    "    Train GNN\n",
    "    '''\n",
    "    filename = f'{output_dir}/eval-{eval}_config-{config}_fold-{fold}_loss_history.txt'\n",
    "    output = open(filename, \"w\")\n",
    "\n",
    "    print('Epoch Training_MSE Validation_MSE', file=output, flush=True)\n",
    "\n",
    "    seed_all(seed)\n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        # Train batches\n",
    "        for train_batch in train_loader:\n",
    "            train_batch = train_batch.to(device)\n",
    "            train_pred, s1_t, s2_t, s3_t, s4_t, s5_t = model(train_batch)\n",
    "            train_true = getattr(train_batch, prop)\n",
    "            train_loss = F.mse_loss(train_pred, train_true)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Evaluate\n",
    "        val_pred, val_true, s1_test, s2_test, s3_test, s4_test, s5_test = test(model, val_loader, prop)\n",
    "        val_loss = F.mse_loss(val_pred, val_true)\n",
    "        print(f'{epoch:d}, {train_loss:e}, {val_loss:e}', file=output, flush=True)\n",
    "    return\n",
    "\n",
    "\n",
    "def test(model, data_loader, prop):\n",
    "    '''\n",
    "    Test GNN\n",
    "    '''\n",
    "    seed_all(seed)\n",
    "    model.eval()\n",
    "    data = next(iter(data_loader)).to(device)\n",
    "    pred, s1_test, s2_test, s3_test, s4_test, s5_test = model(data)\n",
    "    true = getattr(data, prop)\n",
    "    return pred, true, s1_test, s2_test, s3_test, s4_test, s5_test\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    eval = 2\n",
    "    prop = 'strength'\n",
    "    config_dir = './'\n",
    "    config = 0\n",
    "    output_dir = './out/'\n",
    "    seed = 42\n",
    "\n",
    "    if not os.path.exists(config_dir):\n",
    "        os.makedirs(config_dir)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    config_name = config_dir + str(config) + '.json'\n",
    "    with open(config_name, 'r') as h:\n",
    "        params = json.load(h)\n",
    "\n",
    "    l_rate = params['l_rate']\n",
    "    w_decay = params['w_decay']\n",
    "    n_epoch = params['n_epoch']\n",
    "    b_size = params['b_size']\n",
    "    N_fl1 = params['N_fl1']\n",
    "    N_mpl = params['N_mpl']\n",
    "    N_fl2 = params['N_fl2']\n",
    "    N_fl3 = params['N_fl3']\n",
    "\n",
    "    # Set seeds for complete reproducability\n",
    "    seed_all(seed)\n",
    "\n",
    "    # Define the model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    cases = ['Train (A-G) / Test (A-G)',\n",
    "             'Train (A-G) / Test (H-L)']\n",
    "\n",
    "    print('\\n====== Configuration ======')\n",
    "    print(f'Evaluation #{eval}:\\t\\t{cases[eval-2]}')\n",
    "    print(f'Regression task:\\t{prop}')\n",
    "    print(f'Hyper-parameters :\\t{config}.json')\n",
    "\n",
    "# *************************************************************************** #\n",
    "    print('\\n====== Training / Testing ======')\n",
    "    start = time.time()\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        '''Seeding for DataLoaders'''\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "\n",
    "    # Load data\n",
    "    train_loader = torch.load(\"data/train_dataset.pt\",weights_only=False)\n",
    "    \n",
    "    #### Eval = 2 if you're running Evaluation-1 and Eval = 3 if you're running Evaluation-2\n",
    "    if eval==2:\n",
    "        test_loader = torch.load(\"data/test_dataset_2.pt\",weights_only=False)\n",
    "    else:\n",
    "        test_loader = torch.load(\"data/test_dataset_3.pt\",weights_only=False)\n",
    "\n",
    "    # Define model and optimizer\n",
    "    model, optimizer = init_model()\n",
    "    \n",
    "#### Load the checkpoint file if you directly want results else train\n",
    "#     model.load_state_dict(torch.load(\"out/ALL_LIF_eval-2_config-0_prop-strength_fold-NA_checkpoint_AFTER_REVISION_NO_NORM.pth\",map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "\n",
    "    #Train model\n",
    "    train(model, optimizer, train_loader, test_loader,\n",
    "          n_epoch, prop, config, 'NA')\n",
    "\n",
    "    # Test Model\n",
    "    preds, trues, s1_test, s2_test, s3_test, s4_test, s5_test = test(model, test_loader, prop)\n",
    "\n",
    "    # Save model\n",
    "    # torch.save(model.state_dict(), f\"{output_dir}/ALL_LIF_eval-{eval}_config-{config}_prop-{prop}_fold-{'NA'}_checkpoint_AFTER_REVISION_NO_NORM.pth\")\n",
    "\n",
    "    print(f'Processing time: {time.time()-start:.2f} seconds')\n",
    "# *************************************************************************** #\n",
    "    # Report and Visualize predictions\n",
    "    \n",
    "    with open(\"out/stiffness_scaler.pickle\", \"rb\") as f:\n",
    "        stiffness_scaler = pickle.load(f)\n",
    "\n",
    "    with open(\"out/strength_scaler.pickle\", \"rb\") as f:\n",
    "        strength_scaler = pickle.load(f)\n",
    "\n",
    "    with open(\"out/x_scaler.pickle\", \"rb\") as f:\n",
    "        x_scaler = pickle.load(f)\n",
    "    \n",
    "    scaler = {}\n",
    "    scaler['stiffness'] = stiffness_scaler\n",
    "    scaler['strength'] = strength_scaler\n",
    "    scaler['x'] = x_scaler\n",
    "\n",
    "    print('\\n====== RESULTS ======')\n",
    "    preds = scaler[prop].inverse_transform(\n",
    "        preds.detach().detach().cpu().numpy())\n",
    "    trues = scaler[prop].inverse_transform(\n",
    "        trues.detach().detach().cpu().numpy())\n",
    "    meanARE, maxARE = mean_maxARE(preds, trues)\n",
    "\n",
    "    print(\"Spiking activity is :\" )\n",
    "    print(s1_test)\n",
    "    print(s2_test)\n",
    "    print(s3_test)\n",
    "    print(s4_test)\n",
    "    print(s5_test)\n",
    "\n",
    "    print(f'(MeanARE, MaxARE):\\t({meanARE}, {maxARE})')\n",
    "\n",
    "    def plot_results(preds, trues, output_dir, eval, config, prop):\n",
    "\n",
    "        if prop == 'strength':\n",
    "            preds = preds*1000\n",
    "            trues = trues*1000\n",
    "\n",
    "        \n",
    "        '''Plot evaluation results\n",
    "        '''\n",
    "        sns.set(font_scale=1.75)\n",
    "        sns.set_style(\"ticks\")\n",
    "        # fig, ax = plt.subplots(figsize=(8.5, 5.5), dpi=300)\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "        minColor = 0.4\n",
    "        maxColor = 1.00\n",
    "        if prop == 'strength':\n",
    "            cmap = truncate_colormap(plt.get_cmap(\"Greens\"), minColor, maxColor)\n",
    "        else:\n",
    "            cmap = truncate_colormap(plt.get_cmap(\"Blues\"), minColor, maxColor)\n",
    "        col = mcolors.to_hex(cmap(0.5))\n",
    "    \n",
    "        if eval != 2:\n",
    "            x = np.squeeze(trues)\n",
    "            y = np.squeeze(preds)\n",
    "            xy = np.vstack([x, y])\n",
    "            z = gaussian_kde(xy)(xy)\n",
    "            # Sort the points by density, so that the densest points are plotted last\n",
    "            idx = z.argsort()\n",
    "            x, y, z = x[idx], y[idx], z[idx]\n",
    "    \n",
    "            plt.scatter(x,\n",
    "                        y,\n",
    "                        c=z,\n",
    "                        s=20,\n",
    "                        cmap=cmap)\n",
    "        else:\n",
    "            plt.scatter(trues,\n",
    "                        preds,\n",
    "                        s=20,\n",
    "                        ec='k',\n",
    "                        lw=0.5,\n",
    "                        color=col)\n",
    "        if prop == 'strength':\n",
    "            plt.xlabel('True strength (MPa)')\n",
    "            plt.ylabel('Predicted strength (MPa)')\n",
    "            plt.xlim([700, 1220])\n",
    "            plt.ylim([700, 1220])\n",
    "            plt.plot([700, 1220], [700, 1220], '-k', linewidth=2)\n",
    "        else:\n",
    "            plt.xlabel('True modulus (GPa)')\n",
    "            plt.ylabel('Predicted modulus (GPa)')\n",
    "            plt.xlim([110, 152])\n",
    "            plt.ylim([110, 152])\n",
    "            plt.plot([110, 152], [110, 152], '-k', linewidth=2)\n",
    "    \n",
    "        ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n",
    "    \n",
    "        plt.savefig(f'/mnt/sdb1/graphspiking/graph_spiking/PolyGRAPH-main/REVISION_RESULTS/ALL_LIF_eval-{eval}_prop-{prop}_config-{config}.parity.png', dpi=300, bbox_inches=\"tight\")\n",
    "        \n",
    "    plot_results(preds/1000, trues/1000, output_dir, eval, config, prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6349054d",
   "metadata": {},
   "source": [
    "MSE Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907aaa7e-32fb-4a5d-967d-a223a10cf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_norm = scaler[prop].transform(\n",
    "        preds)\n",
    "true_norm = scaler[prop].transform(\n",
    "        trues)\n",
    "\n",
    "np.mean((preds-trues)**2), np.mean((pred_norm-true_norm)**2)*1000, pred_norm.shape, true_norm.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
