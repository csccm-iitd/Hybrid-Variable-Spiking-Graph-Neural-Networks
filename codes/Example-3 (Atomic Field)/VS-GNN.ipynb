{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TdiKeuWnaOB"
   },
   "outputs": [],
   "source": [
    "from os import WIFCONTINUED\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data, DataLoader, DataListLoader\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear, GRUCell\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import PNAConv, BatchNorm, global_mean_pool, DataParallel\n",
    "import snntorch as snn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tFkgiVInhPR"
   },
   "outputs": [],
   "source": [
    "# Set the parameters here\n",
    "batch_size = 3\n",
    "data_path = '/DATA/graphspiking/graph_spiking/Porous/'\n",
    "input_dim = 2\n",
    "num_data = 2000\n",
    "num_layer = 14\n",
    "hidden_dim = 50\n",
    "max_degree = 4\n",
    "epoch = 250    # changed from 500\n",
    "scale_factor = 1e-6\n",
    "timesteps = 1\n",
    "\n",
    "\n",
    "if hidden_dim % 5 != 0:\n",
    "    raise Exception(\"Sorry, not available hidden dimension, need to be multiple of 5\")\n",
    "if num_layer < 1:\n",
    "    raise Exception(\"Sorry, the number of layer is not enough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EvWEY4Jo6Fx"
   },
   "outputs": [],
   "source": [
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNANet, self).__init__()\n",
    "\n",
    "\n",
    "        aggregators = ['mean', 'min', 'max', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.batch_norms  = ModuleList()\n",
    "        self.grus = ModuleList()\n",
    "\n",
    "\n",
    "        # Defining the layers\n",
    "\n",
    "        # Layer 1\n",
    "        beta1_1 = torch.rand(hidden_dim)\n",
    "        beta1_2 = torch.rand(input_dim)\n",
    "        beta1_3 = torch.rand(hidden_dim)\n",
    "        thr1_1 = torch.rand(hidden_dim)\n",
    "        thr1_2 = torch.rand(input_dim)\n",
    "        thr1_3 = torch.rand(hidden_dim)\n",
    "        self.lif1_1 = snn.Leaky(beta = beta1_1,threshold = thr1_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif1_2 = snn.Leaky(beta = beta1_2,threshold = thr1_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif1_3 = snn.Leaky(beta = beta1_3,threshold = thr1_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv1 = PNAConv(in_channels=input_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru1 = GRUCell(input_dim, hidden_dim)\n",
    "        self.batch_norm1 = BatchNorm(hidden_dim)\n",
    "\n",
    "        # Layer 2\n",
    "        beta2_1 = torch.rand(hidden_dim)\n",
    "        beta2_2 = torch.rand(hidden_dim)\n",
    "        beta2_3 = torch.rand(hidden_dim)\n",
    "        thr2_1 = torch.rand(hidden_dim)\n",
    "        thr2_2 = torch.rand(hidden_dim)\n",
    "        thr2_3 = torch.rand(hidden_dim)\n",
    "        self.lif2_1 = snn.Leaky(beta = beta2_1, threshold = thr2_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif2_2 = snn.Leaky(beta = beta2_2, threshold = thr2_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif2_3 = snn.Leaky(beta = beta2_3, threshold = thr2_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv2 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru2 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm2 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 3\n",
    "        beta3_1 = torch.rand(hidden_dim)\n",
    "        beta3_2 = torch.rand(hidden_dim)\n",
    "        beta3_3 = torch.rand(hidden_dim)\n",
    "        thr3_1 = torch.rand(hidden_dim)\n",
    "        thr3_2 = torch.rand(hidden_dim)\n",
    "        thr3_3 = torch.rand(hidden_dim)\n",
    "        self.lif3_1 = snn.Leaky(beta = beta3_1, threshold = thr3_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif3_2 = snn.Leaky(beta = beta3_2, threshold = thr3_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif3_3 = snn.Leaky(beta = beta3_3, threshold = thr3_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv3 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru3 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm3 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 4\n",
    "        beta4_1 = torch.rand(hidden_dim)\n",
    "        beta4_2 = torch.rand(hidden_dim)\n",
    "        beta4_3 = torch.rand(hidden_dim)\n",
    "        thr4_1 = torch.rand(hidden_dim)\n",
    "        thr4_2 = torch.rand(hidden_dim)\n",
    "        thr4_3 = torch.rand(hidden_dim)\n",
    "        self.lif4_1 = snn.Leaky(beta = beta4_1, threshold = thr4_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif4_2 = snn.Leaky(beta = beta4_2, threshold = thr4_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif4_3 = snn.Leaky(beta = beta4_3, threshold = thr4_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv4 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru4 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm4 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 5\n",
    "        beta5_1 = torch.rand(hidden_dim)\n",
    "        beta5_2 = torch.rand(hidden_dim)\n",
    "        beta5_3 = torch.rand(hidden_dim)\n",
    "        thr5_1 = torch.rand(hidden_dim)\n",
    "        thr5_2 = torch.rand(hidden_dim)\n",
    "        thr5_3 = torch.rand(hidden_dim)\n",
    "        self.lif5_1 = snn.Leaky(beta = beta5_1, threshold = thr5_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif5_2 = snn.Leaky(beta = beta5_2, threshold = thr5_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif5_3 = snn.Leaky(beta = beta5_3, threshold = thr5_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv5 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru5 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm5 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 6\n",
    "        beta6_1 = torch.rand(hidden_dim)\n",
    "        beta6_2 = torch.rand(hidden_dim)\n",
    "        beta6_3 = torch.rand(hidden_dim)\n",
    "        thr6_1 = torch.rand(hidden_dim)\n",
    "        thr6_2 = torch.rand(hidden_dim)\n",
    "        thr6_3 = torch.rand(hidden_dim)\n",
    "        self.lif6_1 = snn.Leaky(beta = beta6_1, threshold = thr6_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif6_2 = snn.Leaky(beta = beta6_2, threshold = thr6_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif6_3 = snn.Leaky(beta = beta6_3, threshold = thr6_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv6 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru6 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm6 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 7\n",
    "        beta7_1 = torch.rand(hidden_dim)\n",
    "        beta7_2 = torch.rand(hidden_dim)\n",
    "        beta7_3 = torch.rand(hidden_dim)\n",
    "        thr7_1 = torch.rand(hidden_dim)\n",
    "        thr7_2 = torch.rand(hidden_dim)\n",
    "        thr7_3 = torch.rand(hidden_dim)\n",
    "        self.lif7_1 = snn.Leaky(beta = beta7_1, threshold = thr7_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif7_2 = snn.Leaky(beta = beta7_2, threshold = thr7_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif7_3 = snn.Leaky(beta = beta7_3, threshold = thr7_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv7 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru7 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm7 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 8\n",
    "        beta8_1 = torch.rand(hidden_dim)\n",
    "        beta8_2 = torch.rand(hidden_dim)\n",
    "        beta8_3 = torch.rand(hidden_dim)\n",
    "        thr8_1 = torch.rand(hidden_dim)\n",
    "        thr8_2 = torch.rand(hidden_dim)\n",
    "        thr8_3 = torch.rand(hidden_dim)\n",
    "        self.lif8_1 = snn.Leaky(beta = beta8_1, threshold = thr8_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif8_2 = snn.Leaky(beta = beta8_2, threshold = thr8_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif8_3 = snn.Leaky(beta = beta8_3, threshold = thr8_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv8 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru8 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm8 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 9\n",
    "        beta9_1 = torch.rand(hidden_dim)\n",
    "        beta9_2 = torch.rand(hidden_dim)\n",
    "        beta9_3 = torch.rand(hidden_dim)\n",
    "        thr9_1 = torch.rand(hidden_dim)\n",
    "        thr9_2 = torch.rand(hidden_dim)\n",
    "        thr9_3 = torch.rand(hidden_dim)\n",
    "        self.lif9_1 = snn.Leaky(beta = beta9_1, threshold = thr9_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif9_2 = snn.Leaky(beta = beta9_2, threshold = thr9_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif9_3 = snn.Leaky(beta = beta9_3, threshold = thr9_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv9 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru9 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm9 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 10\n",
    "        beta10_1 = torch.rand(hidden_dim)\n",
    "        beta10_2 = torch.rand(hidden_dim)\n",
    "        beta10_3 = torch.rand(hidden_dim)\n",
    "        thr10_1 = torch.rand(hidden_dim)\n",
    "        thr10_2 = torch.rand(hidden_dim)\n",
    "        thr10_3 = torch.rand(hidden_dim)\n",
    "        self.lif10_1 = snn.Leaky(beta = beta10_1, threshold = thr10_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif10_2 = snn.Leaky(beta = beta10_2, threshold = thr10_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif10_3 = snn.Leaky(beta = beta10_3, threshold = thr10_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv10 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru10 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm10 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 11\n",
    "        beta11_1 = torch.rand(hidden_dim)\n",
    "        beta11_2 = torch.rand(hidden_dim)\n",
    "        beta11_3 = torch.rand(hidden_dim)\n",
    "        thr11_1 = torch.rand(hidden_dim)\n",
    "        thr11_2 = torch.rand(hidden_dim)\n",
    "        thr11_3 = torch.rand(hidden_dim)\n",
    "        self.lif11_1 = snn.Leaky(beta = beta11_1, threshold = thr11_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif11_2 = snn.Leaky(beta = beta11_2, threshold = thr11_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif11_3 = snn.Leaky(beta = beta11_3, threshold = thr11_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv11 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru11 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm11 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 12\n",
    "        beta12_1 = torch.rand(hidden_dim)\n",
    "        beta12_2 = torch.rand(hidden_dim)\n",
    "        beta12_3 = torch.rand(hidden_dim)\n",
    "        thr12_1 = torch.rand(hidden_dim)\n",
    "        thr12_2 = torch.rand(hidden_dim)\n",
    "        thr12_3 = torch.rand(hidden_dim)\n",
    "        self.lif12_1 = snn.Leaky(beta = beta12_1, threshold = thr12_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif12_2 = snn.Leaky(beta = beta12_2, threshold = thr12_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif12_3 = snn.Leaky(beta = beta12_3, threshold = thr12_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv12 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru12 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm12 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 13\n",
    "        beta13_1 = torch.rand(hidden_dim)\n",
    "        beta13_2 = torch.rand(hidden_dim)\n",
    "        beta13_3 = torch.rand(hidden_dim)\n",
    "        thr13_1 = torch.rand(hidden_dim)\n",
    "        thr13_2 = torch.rand(hidden_dim)\n",
    "        thr13_3 = torch.rand(hidden_dim)\n",
    "        self.lif13_1 = snn.Leaky(beta = beta13_1, threshold = thr13_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif13_2 = snn.Leaky(beta = beta13_2, threshold = thr13_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif13_3 = snn.Leaky(beta = beta13_3, threshold = thr13_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv13 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru13 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm13 = BatchNorm(hidden_dim)\n",
    "\n",
    "        #Layer 14\n",
    "        beta14_1 = torch.rand(hidden_dim)\n",
    "        beta14_2 = torch.rand(hidden_dim)\n",
    "        beta14_3 = torch.rand(hidden_dim)\n",
    "        thr14_1 = torch.rand(hidden_dim)\n",
    "        thr14_2 = torch.rand(hidden_dim)\n",
    "        thr14_3 = torch.rand(hidden_dim)\n",
    "        self.lif14_1 = snn.Leaky(beta = beta14_1, threshold = thr14_1, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif14_2 = snn.Leaky(beta = beta14_2, threshold = thr14_2, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.lif14_3 = snn.Leaky(beta = beta14_3, threshold = thr14_3, learn_beta = True, learn_threshold=True, reset_mechanism='zero')\n",
    "        self.conv14 = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg, towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "        self.gru14 = GRUCell(hidden_dim, hidden_dim)\n",
    "        self.batch_norm14 = BatchNorm(hidden_dim)\n",
    "\n",
    "        self.readout = PNAConv(in_channels=hidden_dim, out_channels=1, aggregators=aggregators, scalers=scalers, deg=deg, towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        mem1_1 = self.lif1_1.init_leaky()\n",
    "        mem1_2 = self.lif1_2.init_leaky()\n",
    "        mem1_3 = self.lif1_3.init_leaky()\n",
    "        \n",
    "        mem2_1 = self.lif2_1.init_leaky()\n",
    "        mem2_2 = self.lif2_2.init_leaky()\n",
    "        mem2_3 = self.lif2_3.init_leaky()\n",
    "        \n",
    "        mem3_1 = self.lif3_1.init_leaky()\n",
    "        mem3_2 = self.lif3_2.init_leaky()\n",
    "        mem3_3 = self.lif3_3.init_leaky()\n",
    "        \n",
    "        mem4_1 = self.lif4_1.init_leaky()\n",
    "        mem4_2 = self.lif4_2.init_leaky()\n",
    "        mem4_3 = self.lif4_3.init_leaky()\n",
    "        \n",
    "        mem5_1 = self.lif5_1.init_leaky()\n",
    "        mem5_2 = self.lif5_2.init_leaky()\n",
    "        mem5_3 = self.lif5_3.init_leaky()\n",
    "        \n",
    "        mem6_1 = self.lif6_1.init_leaky()\n",
    "        mem6_2 = self.lif6_2.init_leaky()\n",
    "        mem6_3 = self.lif6_3.init_leaky()\n",
    "        \n",
    "        mem7_1 = self.lif7_1.init_leaky()\n",
    "        mem7_2 = self.lif7_2.init_leaky()\n",
    "        mem7_3 = self.lif7_3.init_leaky()\n",
    "        \n",
    "        mem8_1 = self.lif8_1.init_leaky()\n",
    "        mem8_2 = self.lif8_2.init_leaky()\n",
    "        mem8_3 = self.lif8_3.init_leaky()\n",
    "        \n",
    "        mem9_1 = self.lif9_1.init_leaky()\n",
    "        mem9_2 = self.lif9_2.init_leaky()\n",
    "        mem9_3 = self.lif9_3.init_leaky()\n",
    "        \n",
    "        mem10_1 = self.lif10_1.init_leaky()\n",
    "        mem10_2 = self.lif10_2.init_leaky()\n",
    "        mem10_3 = self.lif10_2.init_leaky()\n",
    "        \n",
    "        mem11_1 = self.lif11_1.init_leaky()\n",
    "        mem11_2 = self.lif11_2.init_leaky()\n",
    "        mem11_3 = self.lif11_3.init_leaky()\n",
    "        \n",
    "        mem12_1 = self.lif12_1.init_leaky()\n",
    "        mem12_2 = self.lif12_2.init_leaky()\n",
    "        mem12_3 = self.lif12_3.init_leaky()\n",
    "        \n",
    "        mem13_1 = self.lif13_1.init_leaky()\n",
    "        mem13_2 = self.lif13_2.init_leaky()\n",
    "        mem13_3 = self.lif13_3.init_leaky()\n",
    "        \n",
    "        mem14_1 = self.lif14_1.init_leaky()\n",
    "        mem14_2 = self.lif14_2.init_leaky()\n",
    "        mem14_3 = self.lif14_3.init_leaky()\n",
    "        \n",
    "        s1_sum = torch.zeros([14]).to(device)\n",
    "        s2_sum = torch.zeros([14]).to(device)\n",
    "        s3_sum = torch.zeros([14]).to(device)\n",
    "\n",
    "        for i in range(timesteps):\n",
    "            # For Layer 1---------------------------------\n",
    "            y = self.conv1(x, edge_index)\n",
    "            \n",
    "            # spiking on y\n",
    "            spk_in1, mem1_1 = self.lif1_1(y, mem1_1)\n",
    "            s1_sum[0] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem1_2 = self.lif1_2(x, mem1_2)\n",
    "            s2_sum[0] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru1(x, y)\n",
    "            \n",
    "            #spiking on relu\n",
    "            z1 = self.batch_norm1(x)\n",
    "            spk_in3, mem1_3 = self.lif1_3(z1, mem1_3)\n",
    "            s3_sum[0] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z1\n",
    "            \n",
    "            # x = F.relu(self.batch_norm1(x))\n",
    "            \n",
    "            # For Layer 2----------------------------------\n",
    "            y = self.conv2(x, edge_index)\n",
    "            \n",
    "            # spiking on y\n",
    "            spk_in1, mem2_1 = self.lif2_1(y, mem2_1)\n",
    "            s1_sum[1] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem2_2 = self.lif2_2(x, mem2_2)\n",
    "            s2_sum[1] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru2(x, y)\n",
    "            #spiking on relu\n",
    "            z2 = self.batch_norm2(x)\n",
    "            spk_in3, mem2_3 = self.lif2_3(z2, mem2_3)\n",
    "            s3_sum[1] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z2\n",
    "\n",
    "            # For Layer 3-----------------------------------\n",
    "            y = self.conv3(x, edge_index)\n",
    "            spk_in1, mem3_1 = self.lif3_1(y, mem3_1)\n",
    "            s1_sum[2] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem3_2 = self.lif3_2(x, mem3_2)\n",
    "            s2_sum[2] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru3(x, y)\n",
    "            #spiking on relu\n",
    "            z3 = self.batch_norm3(x)\n",
    "            spk_in3, mem3_3 = self.lif3_3(z3, mem3_3)\n",
    "            s3_sum[2] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z3\n",
    "\n",
    "            # For Layer 4-----------------------------------\n",
    "            y = self.conv4(x, edge_index)\n",
    "            spk_in1, mem4_1 = self.lif4_1(y, mem4_1)\n",
    "            s1_sum[3] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem4_2 = self.lif4_2(x, mem4_2)\n",
    "            s2_sum[3] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru4(x, y)\n",
    "            #spiking on relu\n",
    "            z4 = self.batch_norm4(x)\n",
    "            spk_in3, mem4_3 = self.lif4_3(z4, mem4_3)\n",
    "            s3_sum[3] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z4\n",
    "\n",
    "            # For Layer 5-----------------------------------\n",
    "            y = self.conv5(x, edge_index)\n",
    "            spk_in1, mem5_1 = self.lif5_1(y, mem5_1)\n",
    "            s1_sum[4] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem5_2 = self.lif5_2(x, mem5_2)\n",
    "            s2_sum[4] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru5(x, y)\n",
    "            #spiking on relu\n",
    "            z5 = self.batch_norm5(x)\n",
    "            spk_in3, mem5_3 = self.lif5_3(z5, mem5_3)\n",
    "            s3_sum[4] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z5\n",
    "\n",
    "            # For Layer 6-----------------------------------\n",
    "            y = self.conv6(x, edge_index)\n",
    "            spk_in1, mem6_1 = self.lif6_1(y, mem6_1)\n",
    "            s1_sum[5] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem6_2 = self.lif6_2(x, mem6_2)\n",
    "            s2_sum[5] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru6(x, y)\n",
    "            #spiking on relu\n",
    "            z6 = self.batch_norm6(x)\n",
    "            spk_in3, mem6_3 = self.lif6_3(z6, mem6_3)\n",
    "            s3_sum[5] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z6\n",
    "            \n",
    "\n",
    "            # For Layer 7-----------------------------------\n",
    "            y = self.conv7(x, edge_index)\n",
    "            spk_in1, mem7_1 = self.lif7_1(y, mem7_1)\n",
    "            s1_sum[6] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem7_2 = self.lif7_2(x, mem7_2)\n",
    "            s2_sum[6] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru7(x, y)\n",
    "            #spiking on relu\n",
    "            z7 = self.batch_norm7(x)\n",
    "            spk_in3, mem7_3 = self.lif7_3(z7, mem7_3)\n",
    "            s3_sum[6] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z7\n",
    "\n",
    "            # For Layer 8-----------------------------------\n",
    "            y = self.conv8(x, edge_index)\n",
    "            spk_in1, mem8_1 = self.lif8_1(y, mem8_1)\n",
    "            s1_sum[7] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem8_2 = self.lif8_2(x, mem8_2)\n",
    "            s2_sum[7] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru8(x, y)\n",
    "            #spiking on relu\n",
    "            z8 = self.batch_norm8(x)\n",
    "            spk_in3, mem8_3 = self.lif8_3(z8, mem8_3)\n",
    "            s3_sum[7] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z8\n",
    "\n",
    "            # For Layer 9-----------------------------------\n",
    "            y = self.conv9(x, edge_index)\n",
    "            spk_in1, mem9_1 = self.lif9_1(y, mem9_1)\n",
    "            s1_sum[8] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem9_2 = self.lif9_2(x, mem9_2)\n",
    "            s2_sum[8] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "             \n",
    "            x = self.gru9(x, y)\n",
    "            #spiking on relu\n",
    "            z9 = self.batch_norm9(x)\n",
    "            spk_in3, mem9_3 = self.lif9_3(z9, mem9_3)\n",
    "            s3_sum[8] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z9\n",
    "\n",
    "            # For Layer 10----------------------------------\n",
    "            y = self.conv10(x, edge_index)\n",
    "            spk_in1, mem10_1 = self.lif10_1(y, mem10_1)\n",
    "            s1_sum[9] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem10_2 = self.lif10_2(x, mem10_2)\n",
    "            s2_sum[9] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru10(x, y)\n",
    "            #spiking on relu\n",
    "            z10 = self.batch_norm10(x)\n",
    "            spk_in3, mem10_3 = self.lif10_3(z10, mem10_3)\n",
    "            s3_sum[9] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z10\n",
    "\n",
    "            # For Layer 11-----------------------------------\n",
    "            y = self.conv11(x, edge_index)\n",
    "            spk_in1, mem11_1 = self.lif11_1(y, mem11_1)\n",
    "            s1_sum[10] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem11_2 = self.lif11_2(x, mem11_2)\n",
    "            s2_sum[10] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru11(x, y)\n",
    "            #spiking on relu\n",
    "            z11 = self.batch_norm11(x)\n",
    "            spk_in3, mem11_3 = self.lif11_3(z11, mem11_3)\n",
    "            s3_sum[10] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z11\n",
    "\n",
    "            # For Layer 12-----------------------------------\n",
    "            y = self.conv12(x, edge_index)\n",
    "            spk_in1, mem12_1 = self.lif12_1(y, mem12_1)\n",
    "            s1_sum[11] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem12_2 = self.lif12_2(x, mem12_2)\n",
    "            s2_sum[11] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru12(x, y)\n",
    "            #spiking on relu\n",
    "            z12 = self.batch_norm12(x)\n",
    "            spk_in3, mem12_3 = self.lif12_3(z12, mem12_3)\n",
    "            s3_sum[11] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z12\n",
    "\n",
    "            # For Layer 13-----------------------------------\n",
    "            y = self.conv13(x, edge_index)\n",
    "            spk_in1, mem13_1 = self.lif13_1(y, mem13_1)\n",
    "            s1_sum[12] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem13_2 = self.lif13_2(x, mem13_2)\n",
    "            s2_sum[12] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru13(x, y)\n",
    "            #spiking on relu\n",
    "            z13 = self.batch_norm13(x)\n",
    "            spk_in3, mem13_3 = self.lif13_3(z13, mem13_3)\n",
    "            s3_sum[12] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z13\n",
    "\n",
    "            # For Layer 14---------------------------------\n",
    "            y = self.conv14(x, edge_index)\n",
    "            spk_in1, mem14_1 = self.lif14_1(y, mem14_1)\n",
    "            s1_sum[13] += torch.sum(spk_in1)/spk_in1.numel()\n",
    "            y = spk_in1*y\n",
    "            # spiking on x\n",
    "            spk_in2, mem14_2 = self.lif14_2(x, mem14_2)\n",
    "            s2_sum[13] += torch.sum(spk_in2)/spk_in2.numel()\n",
    "            x = spk_in2*x\n",
    "            \n",
    "            x = self.gru14(x, y)\n",
    "            #spiking on relu\n",
    "            z14 = self.batch_norm14(x)\n",
    "            spk_in3, mem14_3 = self.lif14_3(z14, mem14_3)\n",
    "            s3_sum[13] += torch.sum(spk_in3)/spk_in3.numel()\n",
    "            x = spk_in3*z14\n",
    "\n",
    "\n",
    "        x = self.readout(x, edge_index)\n",
    "\n",
    "        return x, s1_sum, s2_sum, s3_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fztXbgUQbre7"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, device):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        label = torch.cat([data.y for data in batch]).to(device)\n",
    "        # pred = model(batch)       # commenting as code was changed\n",
    "        pred_list=[]\n",
    "        for data in batch:\n",
    "          pred,s1_t, s2_t, s3_t = model(data.to(device))\n",
    "          pred_list.append(pred)\n",
    "\n",
    "        pred_batch = torch.cat(pred_list)\n",
    "\n",
    "        loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(np.array(batch_loss)),s1_t,s2_t,s3_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTF5-Ap9brcg"
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader, device):\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            label = torch.cat([data.y for data in batch]).to(device)\n",
    "\n",
    "            # pred = model(batch)   # changed\n",
    "            pred_list=[]\n",
    "            for data in batch:\n",
    "              pred,s1_v, s2_v, s3_v = model(data.to(device))\n",
    "              pred_list.append(pred)\n",
    "\n",
    "            pred_batch = torch.cat(pred_list)\n",
    "\n",
    "            loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(np.array(val_loss)), s1_v, s2_v, s3_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "04qt-FUHbrZp",
    "outputId": "1792938d-b78a-434f-8190-a226376752bd"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Read relevant data files\n",
    "    f1 = open(data_path + \"edge.txt\", \"r\")\n",
    "    f2 = open(data_path + \"node_features.txt\", \"r\")\n",
    "    f3 = open(data_path + \"node_labels_sxx.txt\", \"r\")\n",
    "    lines1 = f1.readlines()\n",
    "    lines2 = f2.readlines()\n",
    "    lines3 = f3.readlines()\n",
    "\n",
    "    # Data preprocessing\n",
    "    num_data = num_data\n",
    "    data_list = []\n",
    "    t0 = time.time()\n",
    "    print(\"Number of data processed\\ttime\")\n",
    "    ave = []\n",
    "    for i in range(num_data):\n",
    "        if i % 200 == 0:\n",
    "            print(i, time.time() - t0)\n",
    "        # print(lines1[i])\n",
    "        node1 = [int(idx) for idx in lines1[2 * i].split()[1:]]\n",
    "        node2 = [int(idx) for idx in lines1[2 * i + 1].split()[1:]]\n",
    "        edge_index = torch.tensor([node1, node2], dtype=torch.long)\n",
    "        if input_dim == 1:\n",
    "          xs = [float(idx) for idx in lines2[i].split()[1:]]\n",
    "          node_feature = [[xs[j]] for j in range(len(xs))]\n",
    "        elif input_dim == 2:\n",
    "          xs = [float(idx) for idx in lines2[2 * i].split()[1:]]\n",
    "          ys = [float(idx) for idx in lines2[2 * i + 1].split()[1:]]\n",
    "          node_feature = [[xs[j], ys[j]] for j in range(len(xs))]\n",
    "        elif input_dim == 3:\n",
    "          xs = [float(idx) for idx in lines2[3 * i].split()[1:]]\n",
    "          ys = [float(idx) for idx in lines2[3 * i + 1].split()[1:]]\n",
    "          zs = [float(idx) for idx in lines2[3 * i + 2].split()[1:]]\n",
    "          node_feature = [[xs[j], ys[j], zs[j]] for j in range(len(xs))]\n",
    "        elif input_dim == 4:\n",
    "          xs = [float(idx) for idx in lines2[4 * i].split()[1:]]\n",
    "          ys = [float(idx) for idx in lines2[4 * i + 1].split()[1:]]\n",
    "          zs = [float(idx) for idx in lines2[4 * i + 2].split()[1:]]\n",
    "          ls = [float(idx) for idx in lines2[4 * i + 3].split()[1:]]\n",
    "          node_feature = [[xs[j], ys[j], zs[j], ls[j]] for j in range(len(xs))]\n",
    "        else:\n",
    "           raise Exception(\"Sorry, not available input dimension\")\n",
    "\n",
    "        x = torch.tensor(node_feature, dtype=torch.float)\n",
    "\n",
    "        node_label = [float(idx) * scale_factor for idx in lines3[i].split()[1:]]\n",
    "        y = torch.tensor(node_label, dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data_list.append(data)\n",
    "\n",
    "    mean_value = np.mean(np.array(ave))\n",
    "\n",
    "    batch_size = batch_size\n",
    "    train_loader = DataListLoader(torch.load(\"train_dataset.pt\",weights_only=False))\n",
    "    test_loader = DataListLoader(torch.load(\"test_dataset.pt\",weights_only=False))\n",
    "    val_loader = DataListLoader(torch.load(\"val_dataset.pt\",weights_only=False))\n",
    "\n",
    "    deg = torch.zeros(max_degree, dtype=torch.long)\n",
    "    for data in Train_data:\n",
    "        d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "        deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    torch.cuda.empty_cache()\n",
    "    model = PNANet().to(device)\n",
    "    # model = DataParallel(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=20, min_lr=-1e-5, verbose=True)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    print(\"The number of trainable parameters is:{}\".format(params))\n",
    "\n",
    "\n",
    "    path = './out/'\n",
    "    # Training\n",
    "    print(\"epoch\", \"train loss\", \"validation loss\")\n",
    "\n",
    "    val_loss_curve = []\n",
    "    train_loss_curve = []\n",
    "\n",
    "    for epoch in range(epoch):\n",
    "\n",
    "        # Compute train your model on training data\n",
    "        epoch_loss,s1_t,s2_t, s3_t = train(model, train_loader, optimizer,  device)\n",
    "\n",
    "        # Validate your on validation data\n",
    "        val_loss,s1_v,s2_v, s3_v = validate(model, val_loader, device)\n",
    "\n",
    "\n",
    "        # Record train and loss performance\n",
    "        train_loss_curve.append(epoch_loss)\n",
    "        val_loss_curve.append(val_loss)\n",
    "\n",
    "        # The learning rate scheduler record the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss':epoch_loss,\n",
    "\n",
    "            },\n",
    "            path + str(epoch+1) + \".pt\")\n",
    "        print(epoch, epoch_loss, val_loss)\n",
    "        print(\"s1_t : \",s1_t)\n",
    "        print(\"s2_t : \",s2_t)\n",
    "        print(\"s3_t : \",s3_t)\n",
    "        print(\"s1_v : \",s1_v)\n",
    "        print(\"s2_v : \",s2_v)\n",
    "        print(\"s3_v : \",s3_v)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
