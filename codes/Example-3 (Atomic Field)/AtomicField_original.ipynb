{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh4l_vTndQUp",
        "outputId": "16ca496e-d1a0-407c-a2d5-0110bbd0dccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (2.3.1)\n",
            "Requirement already satisfied: numpy in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (1.23.5)\n",
            "Requirement already satisfied: jinja2 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scipy in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (1.10.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (5.9.0)\n",
            "Requirement already satisfied: requests in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (2.28.1)\n",
            "Requirement already satisfied: scikit-learn in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (4.66.1)\n",
            "Requirement already satisfied: pyparsing in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from jinja2->torch_geometric) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from requests->torch_geometric) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from requests->torch_geometric) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from requests->torch_geometric) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (2.2.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages (from scikit-learn->torch_geometric) (1.1.1)\n",
            "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
            "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIs_zSCibkDN"
      },
      "outputs": [],
      "source": [
        "from os import WIFCONTINUED\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import time\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch_geometric\n",
        "from torch import nn\n",
        "from torch_geometric.data import Data, DataLoader, DataListLoader\n",
        "from torch_geometric.utils import degree\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import ModuleList, Embedding\n",
        "from torch.nn import Sequential, ReLU, Linear, GRUCell\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch_geometric.nn import PNAConv, BatchNorm, global_mean_pool, DataParallel\n",
        "import argparse\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyUzgt3ibrlL"
      },
      "outputs": [],
      "source": [
        "# Arguments\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('-b','--batch_size', default=2, type=int,\n",
        "                    help='batch size')\n",
        "parser.add_argument('-d','--data_path', default='/DATA/graphspiking/data/', type=str,\n",
        "                    help='data path')\n",
        "parser.add_argument('-i','--input_dim', default=2, type=int,\n",
        "                    help='the dimension of coordinates (2D or 3D)')\n",
        "parser.add_argument('-n','--num_data', default=2000, type=int,\n",
        "                    help='the number of all data')\n",
        "parser.add_argument('-l','--num_layer', default=14, type=int,\n",
        "                    help='the number of PNAConv layers')\n",
        "parser.add_argument('-v','--hidden_dim', default=50, type=int,\n",
        "                    help='the hidden dimension of PNANet')\n",
        "parser.add_argument('-m','--max_degree', default=4, type=int,\n",
        "                    help='maximum degree of all nodes')\n",
        "parser.add_argument('-e','--epoch', default=500, type=int,\n",
        "                    help='number of epoch')\n",
        "parser.add_argument('-s','--scale_factor', default=1e-6, type=float,\n",
        "                    help='scale factor for node labels')\n",
        "parser.add_argument('-f')\n",
        "args = parser.parse_args()\n",
        "\n",
        "if args.hidden_dim % 5 != 0:\n",
        "    raise Exception(\"Sorry, not available hidden dimension, need to be multiple of 5\")\n",
        "if args.num_layer < 1:\n",
        "    raise Exception(\"Sorry, the number of layer is not enough\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XHcqAiMzbrhy"
      },
      "outputs": [],
      "source": [
        "class PNANet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PNANet, self).__init__()\n",
        "\n",
        "\n",
        "        aggregators = ['mean', 'min', 'max', 'std']\n",
        "        scalers = ['identity', 'amplification', 'attenuation']\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        self.batch_norms  = ModuleList()\n",
        "        self.grus = ModuleList()\n",
        "\n",
        "        num_layer = args.num_layer\n",
        "        input_dim = args.input_dim\n",
        "        hidden_dim = args.hidden_dim\n",
        "\n",
        "        for i in range(num_layer):\n",
        "            if i == 0:\n",
        "                conv = PNAConv(in_channels=input_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg,\n",
        "                          towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
        "                self.convs.append(conv)\n",
        "                self.grus.append(nn.GRUCell(input_dim, hidden_dim))\n",
        "                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "            else:\n",
        "                conv = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg,\n",
        "                          towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
        "                self.convs.append(conv)\n",
        "                self.grus.append(nn.GRUCell(hidden_dim, hidden_dim))\n",
        "                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.readout = PNAConv(in_channels=hidden_dim, out_channels=1, aggregators=aggregators, scalers=scalers, deg=deg,\n",
        "                          towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for conv, gru, batch_norm in zip(self.convs, self.grus, self.batch_norms):\n",
        "            y = conv(x, edge_index)\n",
        "            x = gru(x, y)\n",
        "            x = F.relu(batch_norm(x))\n",
        "        x = self.readout(x, edge_index)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fztXbgUQbre7"
      },
      "outputs": [],
      "source": [
        "#Train function\n",
        "def train(model, dataloader, optimizer, device):\n",
        "    batch_loss = []\n",
        "    model.train()\n",
        "\n",
        "    for batch in dataloader:\n",
        "        label = torch.cat([data.y for data in batch]).to(device)\n",
        "        # pred = model(batch)       # commenting as code was changed\n",
        "        pred_list=[]\n",
        "        for data in batch:\n",
        "          pred = model(data.to(device))\n",
        "          pred_list.append(pred)\n",
        "\n",
        "        pred_batch = torch.cat(pred_list)\n",
        "\n",
        "        loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_loss.append(loss.item())\n",
        "\n",
        "    return np.mean(np.array(batch_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTF5-Ap9brcg"
      },
      "outputs": [],
      "source": [
        "# Validation function\n",
        "def validate(model, dataloader, device):\n",
        "    val_loss = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            label = torch.cat([data.y for data in batch]).to(device)\n",
        "\n",
        "            # pred = model(batch)   # changed\n",
        "            pred_list=[]\n",
        "            for data in batch:\n",
        "              pred = model(data.to(device))\n",
        "              pred_list.append(pred)\n",
        "\n",
        "            pred_batch = torch.cat(pred_list)\n",
        "\n",
        "            loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
        "            val_loss.append(loss.item())\n",
        "    return np.mean(np.array(val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "04qt-FUHbrZp",
        "outputId": "3f0c3b92-6507-44ca-f989-32c6af68f1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of data processed\ttime\n",
            "0 0.00010752677917480469\n",
            "200 4.213025093078613\n",
            "400 7.891521453857422\n",
            "600 11.482861995697021\n",
            "800 15.20004940032959\n",
            "1000 18.805384159088135\n",
            "1200 22.25380039215088\n",
            "1400 25.793635606765747\n",
            "1600 29.23866558074951\n",
            "1800 32.68548798561096\n",
            "Model architecture:\n",
            "PNANet(\n",
            "  (convs): ModuleList(\n",
            "    (0): PNAConv(2, 50, towers=1, edge_dim=None)\n",
            "    (1-13): 13 x PNAConv(50, 50, towers=5, edge_dim=None)\n",
            "  )\n",
            "  (batch_norms): ModuleList(\n",
            "    (0-13): 14 x BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (grus): ModuleList(\n",
            "    (0): GRUCell(2, 50)\n",
            "    (1-13): 13 x GRUCell(50, 50)\n",
            "  )\n",
            "  (readout): PNAConv(50, 1, towers=1, edge_dim=None)\n",
            ")\n",
            "The number of trainable parameters is:1002563\n",
            "epoch train loss validation loss\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='min')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n",
            "/home/user/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages/torch_geometric/utils/scatter.py:93: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
            "  warnings.warn(f\"The usage of `scatter(reduce='{reduce}')` \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.9755017411176647 1.4325430619716644\n",
            "1 0.5270898751701627 0.728704474568367\n",
            "2 0.31815855404894267 0.6462131260335445\n",
            "3 0.31317959370623744 0.6801067620515824\n",
            "5 0.27319307948063526 0.8169910082221031\n",
            "6 0.27085092765944346 0.6197202572226524\n",
            "7 0.25572684415882185 0.715461283326149\n",
            "8 0.2509342587925494 0.9330663299560547\n",
            "9 0.2466496991698763 0.6403337754309177\n",
            "10 0.24081455906187849 0.7266723623871804\n",
            "11 0.23832622642229712 0.6782606676220894\n",
            "12 0.2343905958506678 0.5696723632514477\n",
            "13 0.23796351459941695 0.7173037105798721\n",
            "14 0.2294299913197756 0.5772407113015652\n",
            "15 0.22585447035995979 0.6905628705024719\n",
            "16 0.225772169978757 0.7081995479762554\n",
            "17 0.2170141119456717 0.5864555698633194\n",
            "18 0.21900726879813842 0.6493200007081031\n",
            "19 0.2317973183068846 0.6260060124099255\n",
            "20 0.21738554454009448 0.5727256074547767\n",
            "21 0.21935982600918838 0.6336311806738376\n",
            "22 0.21384887091682425 0.6120408488810063\n",
            "23 0.21358979263742056 0.5995454309880733\n",
            "24 0.2171960343101195 0.5739412993192673\n",
            "25 0.21108636332476247 0.6641635240614414\n",
            "26 0.20981053836244556 1.4843575723469258\n",
            "27 0.20963567751060638 0.638429839015007\n",
            "28 0.21253697930569096 0.5619401633739471\n",
            "29 0.2112177933952106 0.6280550771951675\n",
            "30 0.20747130888248128 0.6577108643949032\n",
            "31 0.20476225150749086 0.6287569217383862\n",
            "32 0.20449748075301094 0.5779807212948799\n",
            "33 0.20341561419756285 0.70430320546031\n",
            "34 0.21075016949059708 0.5668321576714516\n",
            "35 0.2066739223584799 0.6087241196632385\n",
            "36 0.21097806463816335 0.6575490982830524\n",
            "37 0.20381291145857955 0.5464410425722599\n",
            "38 0.20186400814247982 0.5731841948628426\n",
            "39 0.20008647941319005 0.5980462737381458\n",
            "40 0.20068046039236442 0.7227629525959491\n",
            "41 0.20024605678793575 0.5709838581830263\n",
            "42 0.20264034715880241 0.6084107546508313\n",
            "43 0.1987131974174242 0.6674533388018609\n",
            "44 0.19967419781123422 0.625681380033493\n",
            "45 0.20165039275905916 0.5668557508289814\n",
            "46 0.19803623129347606 0.6220742276310921\n",
            "47 0.19641277985514274 0.6436263447999955\n",
            "48 0.19822105633094908 0.6123498874902725\n",
            "49 0.19561184437280255 0.6081450921297074\n",
            "50 0.19694048892107927 0.6241578164696694\n",
            "51 0.1978511637023517 0.589970966130495\n",
            "52 0.19458173248717295 0.5652829076349736\n",
            "53 0.1933365341089666 0.5679014684259891\n",
            "54 0.19212834733405285 0.6421488711237907\n",
            "55 0.1947254459593179 0.5443554382026196\n",
            "56 0.2074337904521131 0.616529933065176\n",
            "57 0.21219974859617652 0.722409027069807\n",
            "58 0.23207378711285337 0.5584070434421301\n",
            "59 0.21673452233097382 0.7169172635674477\n",
            "60 0.21294269530235657 0.7114399851858616\n",
            "61 0.210739774058706 0.6243718366324902\n",
            "62 0.20414893613063864 0.6015291431546211\n",
            "63 0.20349742375380758 0.5859831526130438\n",
            "64 0.20206781889711106 0.6483449047803879\n",
            "65 0.20088735159619578 0.6025502453744411\n",
            "66 0.20625665531892862 0.6074877291917801\n",
            "67 0.21454526855610312 0.6374601446092129\n",
            "68 0.21259556916808445 0.5535277676582336\n",
            "69 0.20755427694081197 0.6249913918972015\n",
            "70 0.19902865536444422 0.6754285395145416\n",
            "71 0.1991580702065091 0.574851755797863\n",
            "72 0.19759628906979093 0.6128142772614956\n",
            "73 0.19767576245990182 0.6108366475999355\n",
            "74 0.22967200926371983 0.780360895395279\n",
            "75 0.20390801451302001 0.5990956771373749\n",
            "Epoch 00077: reducing learning rate of group 0 to 5.0000e-04.\n",
            "76 0.22470365022974356 0.6154927555471659\n",
            "77 0.19786154839010642 0.5897491323947907\n",
            "78 0.19354291414708963 0.5742118743062019\n",
            "79 0.19111901692513908 0.60105423361063\n",
            "80 0.19042344750004953 0.5887327621877193\n",
            "81 0.18866792738903315 0.5577595111727714\n",
            "82 0.19199064562097193 0.585959499180317\n",
            "83 0.19176305981619018 0.6723858919739724\n",
            "84 0.19331448600228343 0.6398886269330979\n",
            "85 0.18726635872186828 0.6218173806369305\n",
            "86 0.18577172085775862 0.591751438677311\n",
            "87 0.18467435091334794 0.5545441730320454\n",
            "88 0.18344423224550804 0.7009199583530425\n",
            "89 0.18323803789913654 0.6094035525619984\n",
            "90 0.18546550856969718 0.5690831176936626\n",
            "91 0.18140969841275364 0.603629166930914\n",
            "92 0.18210139921201127 0.5415786178410054\n",
            "93 0.179893481809912 0.5442733461409808\n",
            "94 0.18045708778580385 53132460.71399908\n",
            "95 0.18448833420340505 0.5571555069088936\n",
            "96 0.18202074654200778 0.5987980280816555\n",
            "97 0.18041382578866821 44.8294208830595\n",
            "98 0.17873761795793794 0.7496104013174772\n",
            "99 0.18403479337958353 6.360141518115998\n",
            "100 0.18265027092570174 0.5439368008077144\n",
            "101 0.1788384682445654 0.5743773876875639\n",
            "102 0.17841180153590228 0.6123852731287479\n",
            "103 0.17596649937930384 0.5548051795363427\n",
            "104 0.1751769927044266 0.5471520439535379\n",
            "105 0.17761794653920723 0.5929168173670769\n",
            "106 0.1789556608069688 0.6787047787010669\n",
            "107 0.1889238587095003 0.7335762244462967\n",
            "108 0.18330061365916792 0.7001604387164115\n",
            "109 0.17854636723641307 0.6366863586008549\n",
            "110 0.1771727884920048 0.6114503383636475\n",
            "111 0.17744811893972967 0.5598578235507011\n",
            "112 0.17478156543536377 0.6475295881927013\n",
            "Epoch 00114: reducing learning rate of group 0 to 2.5000e-04.\n",
            "113 0.17415120581830187 0.5817402449995279\n",
            "114 0.16917242072456118 0.547339968085289\n",
            "115 0.16702563324277955 0.565226126909256\n",
            "116 0.16615004782697984 0.6105429595708847\n",
            "117 0.16547686247221594 0.5791312035918236\n",
            "118 0.1655921873463584 0.5669368329644203\n",
            "119 0.16559763672029865 0.623583847284317\n",
            "120 0.16559648657949375 1306196.724907255\n",
            "121 0.16468747198714742 0.5778704243898392\n",
            "122 0.1643605016145323 0.5354298250377179\n",
            "123 0.16393353744942163 0.7257394227385521\n",
            "124 0.16365366432749268 0.5753955693542957\n",
            "125 0.1625629815978131 0.5830869616568088\n",
            "126 0.16162707499893647 0.5493654996156693\n",
            "127 0.16194298261310905 0.5669785705208779\n",
            "128 0.1616800078776266 0.5638253622502089\n",
            "129 0.16105835149397274 0.6112007744610309\n",
            "130 0.16104710866164948 0.5753025359660388\n",
            "131 0.16027115418442658 0.5817016340792179\n",
            "132 0.16113537051714957 0.552987983673811\n",
            "133 0.16172384729874986 0.6239198109507561\n",
            "134 0.1616109246547733 0.6036038999259472\n",
            "135 0.15926564579230867 183.2509084381163\n",
            "136 0.15903319109696895 0.5739048825949431\n",
            "137 0.1609889788519857 0.563439228758216\n",
            "138 0.16016239902802876 131.87691746070982\n",
            "139 0.15946756758128425 236.53375452749432\n",
            "140 0.15857870278248032 0.5467730371654034\n",
            "141 0.1578425136754023 0.5608342431485653\n",
            "142 0.15741002642216959 0.5747384667396546\n",
            "Epoch 00144: reducing learning rate of group 0 to 1.2500e-04.\n",
            "143 0.15894308973902038 5.322297606766224\n",
            "144 0.16315847396185357 297.4297607693076\n",
            "145 0.1576119536979656 0.5517781361937523\n",
            "146 0.15565070884435306 0.5600141461193562\n",
            "147 0.153427111145242 0.5757369892299176\n",
            "148 0.15280777064684245 0.5390362574160099\n",
            "149 0.15240746988848383 0.6418299639225006\n",
            "150 0.15178128058556467 0.7075706326961517\n",
            "151 0.1514062627185402 0.5591682225465775\n",
            "152 0.15128002821866954 0.5518601078540086\n",
            "153 0.15081640795538467 0.5407105846703053\n",
            "154 0.15075657119608618 0.5576323945820332\n",
            "155 0.15046627011615782 0.5582815235853196\n",
            "156 0.15002279103201416 0.5822343328595161\n",
            "157 0.14987542602200327 0.5539678218960762\n",
            "158 0.1501722780319064 0.5514041966199875\n",
            "159 0.14977562578661102 0.5682551537454128\n",
            "160 0.1495828799956611 0.5759941586107016\n",
            "161 0.14890903236810119 0.5764369992911815\n",
            "162 0.14953658697893843 0.5779039176553488\n",
            "163 0.14903432433559957 0.5801167860627174\n",
            "Epoch 00165: reducing learning rate of group 0 to 6.2500e-05.\n",
            "164 0.14859236458116876 0.5601627965271473\n",
            "165 0.14653553510883024 0.571122917830944\n",
            "166 0.14604356590724948 0.5562588433921337\n",
            "167 0.14570421797822097 0.5685559757053852\n",
            "168 0.1455933658203243 0.5389074097573757\n",
            "169 0.14554536685048203 0.547552361190319\n",
            "170 0.1454605337944148 0.5985097929835319\n",
            "171 0.14513540416157672 0.545035566240549\n",
            "172 0.14495806410443038 0.6103860263526439\n",
            "173 0.14494316816695832 0.6020561990141868\n",
            "174 0.14484625563857012 0.5675750659406185\n",
            "175 0.14460647477236177 0.5589010640978813\n",
            "176 0.14444576391218497 0.5680047386884689\n",
            "177 0.14423021735290864 0.5523639431595803\n",
            "178 0.14438508629798888 0.6268091922998429\n",
            "179 0.1443125129278217 0.5504824426770211\n",
            "180 0.1447397623836462 0.5374046577513218\n",
            "181 0.14406571433147683 0.553387712687254\n",
            "182 0.14358265092496628 0.6169100873172283\n",
            "183 0.14348700754311203 0.5428902477025985\n",
            "184 0.14342281303501556 0.5458530233800412\n",
            "Epoch 00186: reducing learning rate of group 0 to 3.1250e-05.\n",
            "185 0.1434528153692372 0.5686012762784958\n",
            "186 0.1422020982405437 0.5672983484715224\n",
            "187 0.14195491097601395 0.5497682026028633\n",
            "188 0.1418837410291391 0.5835336036980152\n",
            "189 0.14169337223084377 0.5445480962097645\n",
            "190 0.14178421652776055 0.5327253542840481\n",
            "191 0.14161330499752825 0.5528169071674347\n",
            "192 0.14156902680705702 0.5466724876314402\n",
            "193 0.1418207586709676 0.5530453319847584\n",
            "194 0.1416607995584075 0.5436241407692433\n",
            "195 0.14148226932555968 0.540683204382658\n",
            "196 0.14132521906102608 0.5602091911435128\n",
            "197 0.14132711860816927 0.5659795606136322\n",
            "198 0.14123471733049622 0.5991104687750339\n",
            "199 0.1410973021108657 0.551924895197153\n",
            "200 0.14102887454509203 0.5590053276717663\n",
            "201 0.14100578238655415 0.6168685530126095\n",
            "202 0.14091905408538877 0.5602710629999638\n",
            "203 0.14089733865311635 0.5445535518229008\n",
            "204 0.14086616697721183 0.5766133145987987\n",
            "205 0.14076417187162277 0.561120483726263\n",
            "206 0.1406566548327516 0.5634401795268059\n",
            "207 0.1405924419267103 0.623468466848135\n",
            "208 0.14066474665056117 0.5667986260354518\n",
            "209 0.14043066034891777 0.6214054572582245\n",
            "210 0.14040989270034646 0.5335683608800172\n",
            "Epoch 00212: reducing learning rate of group 0 to 1.5625e-05.\n",
            "211 0.1404409680129694 0.688632543683052\n",
            "212 0.13978890613253628 0.5479958618432283\n",
            "213 0.1397706291116109 0.5522136391699314\n",
            "214 0.13951674028033656 0.5427659337222576\n",
            "215 0.13968963772923287 0.6262192164361476\n",
            "216 0.13958635597755867 0.5425433902442456\n",
            "217 0.1395414879884837 0.5836532306671143\n",
            "218 0.13958437675909538 0.5705488386005163\n",
            "219 0.13943846457371753 0.5504094235599041\n",
            "220 0.13947538807755336 0.6115542551875115\n",
            "221 0.1393691430753097 0.5568283946812153\n",
            "222 0.1394301841880328 0.5710622686147689\n",
            "223 0.13929884522887212 0.5766591882705688\n",
            "224 0.13928617141209543 0.5478594439476728\n",
            "225 0.13937494935434577 0.7582956923544407\n",
            "226 0.13915723873452018 0.560013959929347\n",
            "227 0.13925176739360073 0.5529299904406071\n",
            "228 0.13919782525294327 0.5648403288424015\n",
            "229 0.13919690959288605 0.562579420953989\n",
            "230 0.1390611564005459 0.5827425406873226\n",
            "231 0.13903144904744943 0.5546116894483566\n",
            "Epoch 00233: reducing learning rate of group 0 to 7.8125e-06.\n",
            "232 0.13895269285155726 0.5564906619489193\n",
            "233 0.13873632284213921 0.5661478078365326\n",
            "234 0.13870252205391548 0.5480656554549932\n",
            "235 0.13871544088170465 0.5446247638761997\n",
            "236 0.13866616116603836 0.6386780133843422\n",
            "237 0.13858493381263023 0.5588775938749313\n",
            "238 0.1386008895561099 0.5643800124526024\n",
            "239 0.13853806673083455 0.63142025411129\n",
            "240 0.13865321912569925 0.5409604210406542\n",
            "241 0.1386842579748814 0.6563814662396907\n",
            "242 0.13851483266833903 0.5578431434929371\n",
            "243 0.13852329667724136 0.594128771647811\n",
            "244 0.1384093027362334 0.5472109876573086\n",
            "245 0.13845731806675238 0.5467738422006369\n",
            "246 0.13836160157008895 0.7736367863416672\n",
            "247 0.1384913123744939 0.5449524752795696\n",
            "248 0.1383022237901709 0.5677904628217221\n",
            "249 0.1383727166382596 0.5537022645771503\n",
            "250 0.13842351098079234 0.5477283017337322\n",
            "251 0.1383742193947546 0.5409333700686694\n",
            "252 0.13829824396276047 0.5466268064081669\n",
            "Epoch 00254: reducing learning rate of group 0 to 3.9063e-06.\n",
            "253 0.13834709769979653 0.5549101138114929\n",
            "254 0.1381280151935893 0.5491790412366391\n",
            "255 0.13812224613768714 0.5516144028306007\n",
            "256 0.13820021691532539 0.5636250792443752\n",
            "257 0.13823976070121197 0.5657349213957786\n",
            "258 0.1381749064934307 0.5472122432291507\n",
            "259 0.13822398693832968 0.5445246624946595\n",
            "260 0.1381685994153044 0.5457856347411871\n",
            "261 0.13811028442012943 0.563803308159113\n",
            "262 0.13800147678570024 0.5839979891479016\n",
            "263 0.13798398039869167 0.6698729246854782\n",
            "264 0.13813148104253092 0.5474020903557539\n",
            "265 0.13804901428553942 0.5681757253408432\n",
            "266 0.13815659942105413 0.586422808021307\n",
            "267 0.13801787028920703 0.5438609778881073\n",
            "268 0.1380718685177687 0.5645959511399269\n",
            "269 0.1379891517442385 0.5559372551739216\n",
            "270 0.138080712662278 0.5490176787972451\n",
            "271 0.13797659676854632 0.6103693190217018\n",
            "272 0.13805225312610023 0.5515323054045439\n",
            "273 0.13814957404642234 0.549428344219923\n",
            "Epoch 00275: reducing learning rate of group 0 to 1.9531e-06.\n",
            "274 0.13801379431172142 0.5489265322685242\n",
            "275 0.13788417530179556 0.546622849702835\n",
            "276 0.13800173481460662 0.5627779154479504\n",
            "277 0.13782847580533208 0.6462310589849949\n",
            "278 0.13798599226666347 0.6932061544060707\n",
            "279 0.13793271741497198 0.5693815748393536\n",
            "280 0.1379233712885928 0.5659502912312746\n",
            "281 0.1380216549619633 0.6500062042474747\n",
            "282 0.1378927210315929 0.58106529019773\n",
            "283 0.13786759538176868 0.5828046533465385\n",
            "284 0.1379527305402527 0.5761459904909134\n",
            "285 0.13786842029242377 0.5744129751622676\n",
            "286 0.1379857236432976 0.661867695748806\n",
            "287 0.13786832699685225 0.5506893615424633\n",
            "288 0.13784817412961275 0.5638044647872448\n",
            "289 0.13777693025302143 0.5693322393298149\n",
            "290 0.13779683454061992 0.6155430229008197\n",
            "291 0.13781934967297796 0.5829705557227135\n",
            "292 0.13783987245821794 0.563434177711606\n",
            "293 0.13784093164439712 0.5448069934546947\n",
            "294 0.13782027160615792 0.6422109360992908\n",
            "Epoch 00296: reducing learning rate of group 0 to 9.7656e-07.\n",
            "295 0.1378453119952298 0.5611911854147911\n",
            "296 0.13779803073100214 0.6398080752789974\n",
            "297 0.13789632029332485 0.5677063873410225\n",
            "298 0.13781361339879888 0.5622961650043726\n",
            "299 0.13779902795861873 0.5691314560174942\n",
            "300 0.13776748501762215 0.5835115253925324\n",
            "301 0.1378035018905731 0.5619519052654505\n",
            "302 0.1378063623064996 0.557360109090805\n",
            "303 0.13767392325314826 0.5693312539160251\n",
            "304 0.13777651327329554 0.5439307504892349\n",
            "305 0.13775431874939906 0.5469053338468075\n",
            "306 0.1378112410394741 0.5639153248071671\n",
            "307 0.1377541965605425 0.5593206290900707\n",
            "308 0.13776277426796565 0.5743459677696228\n",
            "309 0.13775162014245454 0.5500812995433807\n",
            "310 0.13776699826221114 0.6354611025750637\n",
            "311 0.13775341413516018 0.570124678760767\n",
            "312 0.1377944349417729 0.6704635051265359\n",
            "313 0.13773150151628735 0.5498330196738244\n",
            "314 0.1376895245081479 0.5838857978582382\n",
            "315 0.137761447263682 0.5519494897127152\n",
            "Epoch 00317: reducing learning rate of group 0 to 4.8828e-07.\n",
            "316 0.13770835067544665 0.5448556213825941\n",
            "317 0.13761250745970754 0.5655956538021565\n",
            "318 0.1376368099431108 0.5442122244834899\n",
            "319 0.1377485156149071 0.5975549991428852\n",
            "320 0.13771701393183322 0.5534744623303414\n",
            "321 0.13773875302003163 0.5672972057759762\n",
            "322 0.1377164106742878 0.6512115208804607\n",
            "323 0.1377587026450783 0.548355480581522\n",
            "324 0.13772243633600217 0.5500197814404965\n",
            "325 0.13766393364639953 0.5488734091818332\n",
            "326 0.13765948871416706 0.6018910683691502\n",
            "327 0.1377134258236869 0.5575608615577221\n",
            "328 0.13786678874266467 0.5548977631330491\n",
            "329 0.1376338608815734 0.5977520602941513\n",
            "330 0.1376408230726208 0.5513422289490699\n",
            "331 0.1376503947576774 0.56396734431386\n",
            "332 0.1377512717546363 0.5931149551272392\n",
            "333 0.13773646208357865 0.5722129292786121\n",
            "334 0.13775122129923797 0.5442887308448553\n",
            "335 0.1377162330003921 0.5572209800034762\n",
            "336 0.1378662616486794 0.5669669955968857\n",
            "Epoch 00338: reducing learning rate of group 0 to 2.4414e-07.\n",
            "337 0.13777495973577192 0.5477077616751194\n",
            "338 0.13771824504182276 0.5497378607839346\n",
            "339 0.13774666366433458 0.6063383831828832\n",
            "340 0.13764126802960944 0.547169980108738\n",
            "341 0.13765331642968315 0.5458526493608952\n",
            "342 0.1376867914331212 0.7048412881791591\n",
            "343 0.1376181052991056 0.5822038438171149\n",
            "344 0.13757743640669753 0.5677749238908291\n",
            "345 0.13763298259050186 0.5502588388323784\n",
            "346 0.13769071570065405 0.558443633466959\n",
            "347 0.13764735856642282 0.5539709285646677\n",
            "348 0.13774466518844877 0.5605665209889412\n",
            "349 0.13768905316762228 0.547021717056632\n",
            "350 0.13770351796810115 0.5551920232921839\n",
            "351 0.13765664828542087 0.5987768943607807\n",
            "352 0.1376922718062997 0.6098257706314325\n",
            "353 0.13761684479557776 0.5645884461700916\n",
            "354 0.13775366183503399 0.5880471573770046\n",
            "355 0.13771729622834494 0.5792556296288968\n",
            "356 0.13771152724611707 0.6089058989286422\n",
            "357 0.13760906619285898 0.5503735098987818\n",
            "Epoch 00359: reducing learning rate of group 0 to 1.2207e-07.\n",
            "358 0.1376533771256384 0.5419399254024029\n",
            "359 0.13778311443648167 0.5563598628342151\n",
            "360 0.13769277734415872 0.6354837769269943\n",
            "361 0.13760459835241948 0.5885608759522438\n",
            "362 0.13763952922608172 0.5750672373175622\n",
            "363 0.13766366207074107 0.5478444254398346\n",
            "364 0.13776496487536602 0.5468586674332618\n",
            "365 0.13767225576099007 0.5460353939235211\n",
            "366 0.13760741368767673 0.6071587613970041\n",
            "367 0.13771876948054082 0.5625736899673939\n",
            "368 0.1375903891735444 0.5488073396682739\n",
            "369 0.1375858343691964 0.5480690957605838\n",
            "370 0.1376631112110668 0.5514610615372658\n",
            "371 0.1375596978591888 0.5875317986309528\n",
            "372 0.13757793897496803 0.5694222550094128\n",
            "373 0.13766987723830554 0.5709596587717534\n",
            "374 0.1376391671827462 0.5506482595205306\n",
            "375 0.13772850356064736 0.5941062273085117\n",
            "376 0.13755880839523993 0.5424570536613464\n",
            "377 0.1376390378503129 0.7323772639036179\n",
            "378 0.1376562789887456 0.5539974665641785\n",
            "Epoch 00380: reducing learning rate of group 0 to 6.1035e-08.\n",
            "379 0.13764119332822572 0.5445538079738617\n",
            "380 0.13783680372139706 0.5573133901506663\n",
            "381 0.1375368405905153 0.6028458535671234\n",
            "382 0.137747546459016 0.5660110366344452\n",
            "383 0.13761231098257537 0.5570535673201085\n",
            "384 0.1376514711270907 0.6385461471974849\n",
            "385 0.13769488413884703 0.5525207722187042\n",
            "386 0.137731180837644 0.6013212338089943\n",
            "387 0.13772374912364674 0.553121577501297\n",
            "388 0.13773100518688028 0.5648011767864227\n",
            "389 0.13759636470532444 0.5971950016170741\n",
            "390 0.13763448214291463 0.5689663869142533\n",
            "391 0.137580253638693 0.5483816262334585\n",
            "392 0.13756974933935062 0.6345220051705838\n",
            "393 0.13756675317151737 0.6157090245187282\n",
            "394 0.13761885686178824 0.6629350018501282\n",
            "395 0.1376272784399667 0.5547515252232551\n",
            "396 0.1377425050311389 0.5661109310388565\n",
            "397 0.1376195617958105 0.6103543032705784\n",
            "398 0.13766503447866332 0.5489836391806603\n",
            "Epoch 00401: reducing learning rate of group 0 to 3.0518e-08.\n",
            "400 0.13764394326068993 0.5513009131699801\n",
            "401 0.13758273078527833 0.5825951670110225\n",
            "402 0.13766093845578975 0.5529469834268093\n",
            "403 0.13767879748118242 0.5471822501718998\n",
            "404 0.13767687936812373 0.542855026870966\n",
            "405 0.13772751824813895 0.6095619282126427\n",
            "406 0.13768948051113902 0.5806023362278938\n",
            "407 0.13765815761539021 0.5656154495477677\n",
            "408 0.13767973898444325 0.5950881008803844\n",
            "409 0.13766529499553143 0.6532558800280094\n",
            "410 0.13772091813851148 0.5447828172147274\n",
            "411 0.1375396290354963 0.5486447530239821\n",
            "412 0.1376917818075578 0.5909569188952446\n",
            "413 0.13762066325438874 0.5591061937808991\n",
            "414 0.13781665057675646 0.5557057103514671\n",
            "415 0.13773669883741865 0.6359864029288292\n",
            "416 0.13768636969849468 0.5496249958127737\n",
            "417 0.13768530777555757 0.5741604347527027\n",
            "418 0.1376702794279637 0.5731640776991844\n",
            "419 0.1376532549051834 0.6736578498780728\n",
            "420 0.1376455623846102 0.5597422108799219\n",
            "Epoch 00422: reducing learning rate of group 0 to 1.5259e-08.\n",
            "421 0.1377030672335864 0.5444811599701643\n",
            "422 0.1376114870128887 0.565853081047535\n",
            "423 0.1376478522600207 0.5750005519390107\n",
            "424 0.1377290990330013 0.549895489513874\n",
            "425 0.1376699349523655 0.5876203066110611\n",
            "426 0.13764836200280114 0.5503119017183781\n",
            "427 0.13760203648092492 0.5498126852512359\n",
            "428 0.13767171447531187 0.5649842986464501\n",
            "429 0.13781751856157956 0.5726414211094379\n",
            "430 0.13761047575556273 0.5612263703346252\n",
            "431 0.13757524491021675 0.5591548454761505\n",
            "432 0.13762580872912492 0.5548616415262222\n",
            "433 0.13763720991805062 0.6041319189965725\n",
            "434 0.13766995886540306 0.6523591476678848\n",
            "435 0.1377107782748395 0.5672565987706184\n",
            "436 0.13767307979853025 0.5594577753543853\n",
            "437 0.13755421172195514 0.5460869173705578\n",
            "438 0.13755529419918144 0.5494666829705238\n",
            "439 0.13759869512636214 0.5744170649349689\n",
            "440 0.1375781735102646 0.5525468879193067\n",
            "441 0.13756593136848616 0.5600530096888542\n",
            "442 0.13764104490634055 0.5969117768108845\n",
            "443 0.13773053723636883 0.5857764340937137\n",
            "444 0.1376974096930852 0.5492161682248116\n",
            "445 0.1377435083171752 0.5575025106966496\n",
            "446 0.1376386261026242 0.5641120845079421\n",
            "447 0.13775285190170897 0.5408299420773983\n",
            "448 0.137693252047923 0.5476242525875569\n",
            "449 0.13763371995450663 0.6323731037974357\n",
            "450 0.1377103686818321 0.5550092643499375\n",
            "451 0.13777609272926514 0.5515466497838497\n",
            "452 0.13761086635890285 0.5524654059112072\n",
            "453 0.1375873198370183 0.5587689293920994\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Read relevant data files\n",
        "    f1 = open(args.data_path + \"edge.txt\", \"r\")\n",
        "    f2 = open(args.data_path + \"node_features.txt\", \"r\")\n",
        "    f3 = open(args.data_path + \"node_labels_sxx.txt\", \"r\")\n",
        "    lines1 = f1.readlines()\n",
        "    lines2 = f2.readlines()\n",
        "    lines3 = f3.readlines()\n",
        "\n",
        "    # Data preprocessing\n",
        "    num_data = args.num_data\n",
        "    data_list = []\n",
        "    t0 = time.time()\n",
        "    print(\"Number of data processed\\ttime\")\n",
        "    ave = []\n",
        "    for i in range(num_data):\n",
        "        if i % 200 == 0:\n",
        "            print(i, time.time() - t0)\n",
        "        node1 = [int(idx) for idx in lines1[2 * i].split()[1:]]\n",
        "        node2 = [int(idx) for idx in lines1[2 * i + 1].split()[1:]]\n",
        "        edge_index = torch.tensor([node1, node2], dtype=torch.long)\n",
        "        if args.input_dim == 1:\n",
        "          xs = [float(idx) for idx in lines2[i].split()[1:]]\n",
        "          node_feature = [[xs[j]] for j in range(len(xs))]\n",
        "        elif args.input_dim == 2:\n",
        "          xs = [float(idx) for idx in lines2[2 * i].split()[1:]]\n",
        "          ys = [float(idx) for idx in lines2[2 * i + 1].split()[1:]]\n",
        "          node_feature = [[xs[j], ys[j]] for j in range(len(xs))]\n",
        "        elif args.input_dim == 3:\n",
        "          xs = [float(idx) for idx in lines2[3 * i].split()[1:]]\n",
        "          ys = [float(idx) for idx in lines2[3 * i + 1].split()[1:]]\n",
        "          zs = [float(idx) for idx in lines2[3 * i + 2].split()[1:]]\n",
        "          node_feature = [[xs[j], ys[j], zs[j]] for j in range(len(xs))]\n",
        "        elif args.input_dim == 4:\n",
        "          xs = [float(idx) for idx in lines2[4 * i].split()[1:]]\n",
        "          ys = [float(idx) for idx in lines2[4 * i + 1].split()[1:]]\n",
        "          zs = [float(idx) for idx in lines2[4 * i + 2].split()[1:]]\n",
        "          ls = [float(idx) for idx in lines2[4 * i + 3].split()[1:]]\n",
        "          node_feature = [[xs[j], ys[j], zs[j], ls[j]] for j in range(len(xs))]\n",
        "        else:\n",
        "           raise Exception(\"Sorry, not available input dimension\")\n",
        "\n",
        "        x = torch.tensor(node_feature, dtype=torch.float)\n",
        "        node_label = [float(idx) * args.scale_factor for idx in lines3[i].split()[1:]]\n",
        "        y = torch.tensor(node_label, dtype=torch.float)\n",
        "        data = Data(x=x, edge_index=edge_index, y=y)\n",
        "        # m = data.x\n",
        "        # print(m)\n",
        "        data_list.append(data)\n",
        "\n",
        "    mean_value = np.mean(np.array(ave))\n",
        "    Train_data, Test_data = train_test_split(data_list, test_size = 0.275, random_state=42)\n",
        "    Train_data, Val_data = train_test_split(Train_data, test_size = 0.035, random_state=42)\n",
        "\n",
        "    batch_size = args.batch_size\n",
        "    train_loader = DataListLoader(Train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataListLoader(Test_data, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataListLoader(Val_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    deg = torch.zeros(args.max_degree, dtype=torch.long)\n",
        "    for data in Train_data:\n",
        "        d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
        "        deg += torch.bincount(d, minlength=deg.numel())\n",
        "\n",
        "    device = \"cuda:0\"\n",
        "    torch.cuda.empty_cache()\n",
        "    model = PNANet().to(device)\n",
        "    # model = DataParallel(model)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=20, min_lr=-1e-5, verbose=True)\n",
        "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
        "    print(\"Model architecture:\")\n",
        "    print(model)\n",
        "    print(\"The number of trainable parameters is:{}\".format(params))\n",
        "\n",
        "\n",
        "    path = '/DATA/graphspiking/ckpt_orig_2/'\n",
        "    # Training\n",
        "    print(\"epoch\", \"train loss\", \"validation loss\")\n",
        "\n",
        "    val_loss_curve = []\n",
        "    train_loss_curve = []\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "\n",
        "        # Compute train your model on training data\n",
        "        epoch_loss = train(model, train_loader, optimizer,  device=0)\n",
        "\n",
        "        # Validate your on validation data\n",
        "        val_loss = validate(model, val_loader, device=0)\n",
        "\n",
        "\n",
        "        # Record train and loss performance\n",
        "        train_loss_curve.append(epoch_loss)\n",
        "        val_loss_curve.append(val_loss)\n",
        "\n",
        "        # The learning rate scheduler record the validation loss\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 20 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict':model.state_dict(),\n",
        "                'optimizer_state_dict':optimizer.state_dict(),\n",
        "                'loss':epoch_loss,\n",
        "\n",
        "            },\n",
        "            path + str(epoch+1) + \".pt\")\n",
        "        print(epoch, epoch_loss, val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHbLrM_NT7fa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5OHcFoNT7fa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZa8rgg5T7fa"
      },
      "outputs": [],
      "source": [
        "torch.save(model,'/DATA/graphspiking/ckpt_orig_2/453.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNV6ofcbT7fb"
      },
      "outputs": [],
      "source": [
        "model=torch.load('/DATA/graphspiking/ckpt_orig_2/453.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Vdns_1nT7fb",
        "outputId": "b770e7a7-64e8-4a12-b6c2-1edb8eaa60df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1376843332635638"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysuEJLsjT7fb"
      },
      "outputs": [],
      "source": [
        "torch.save({\n",
        "    'epoch': 453,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}, '/DATA/graphspiking/ckpt_orig_2/453.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eYWk4zTT7fb"
      },
      "outputs": [],
      "source": [
        "# Load the model checkpoint at epoch 453\n",
        "checkpoint = torch.load('/DATA/graphspiking/ckpt_orig_2/453.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "start_epoch = checkpoint['epoch'] + 1  # Start from the next epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuz9983xT7fb",
        "outputId": "fdb750c1-6162-4ad9-8494-01716d2eb1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch train loss validation loss\n",
            "454 0.137649029729655 0.5549473875015974\n",
            "455 0.1377469482352691 0.5513487920165062\n",
            "456 0.1376733301420297 0.5938590614497662\n",
            "457 0.13769823523504393 0.5645996236801147\n",
            "458 0.13777859871009632 0.5448531959950924\n",
            "459 0.13762232600977378 0.5570385427027941\n",
            "460 0.13760328427655621 0.5548129042983055\n",
            "461 0.13763012084527873 0.5482439592480659\n",
            "462 0.1378346759716182 0.5602750903367997\n",
            "463 0.13774404184975927 0.5751591975986957\n",
            "464 0.13766275910427794 0.5440929931402206\n",
            "465 0.1376417409069836 0.5723896725475788\n",
            "466 0.13768315743788012 0.5468115486204624\n",
            "467 0.1376068274527123 0.5660721746087074\n",
            "468 0.13761295611537727 0.5533799427747726\n",
            "469 0.1377065071776243 0.5499404111504554\n",
            "470 0.13767208398585873 0.5813687826693058\n",
            "471 0.1375731175565826 0.5919675751030445\n",
            "472 0.13770013745874166 0.5684518429636956\n",
            "473 0.1376341019517609 0.6084320424497127\n",
            "474 0.13771640879176889 0.5524491652846336\n",
            "475 0.13758622834924608 0.5999023343622685\n",
            "476 0.13763504669070245 0.5676434800028801\n",
            "477 0.13774581860418297 0.6107957665622235\n",
            "478 0.13774826798987175 0.545719242990017\n",
            "479 0.13762238426267037 0.5696809263527394\n",
            "480 0.13761165401865064 0.7194393622875214\n",
            "481 0.13768689181056937 0.5842913138121366\n",
            "482 0.13762269853913625 0.6328336854279041\n",
            "483 0.13761266713862175 0.5526223757863045\n",
            "484 0.13768944877532444 0.5482551857084036\n",
            "485 0.13760581915167028 0.5517395061254501\n",
            "486 0.13772676063335634 0.5473014810681343\n",
            "487 0.13769278933726517 0.5459840344637632\n",
            "488 0.137654338582818 0.5521773737668991\n",
            "489 0.13757502464111895 0.5581398469209671\n",
            "490 0.1375766592299832 0.5605401203036309\n",
            "491 0.1377254535054921 0.6030482970178127\n",
            "492 0.13771349123612578 0.5634639569371939\n",
            "493 0.137555287516942 0.5993236533552408\n",
            "494 0.13760373337294107 0.5552616047859192\n",
            "495 0.13773540539201348 0.6576677110791206\n",
            "496 0.13772195605760706 0.5455109499394893\n",
            "497 0.13762726084462235 0.552573309391737\n",
            "498 0.13769245773454064 0.5595227560400963\n",
            "499 0.13771009176370821 0.5870911142230034\n"
          ]
        }
      ],
      "source": [
        "  path = '/DATA/graphspiking/ckpt_orig_2/'\n",
        "\n",
        "  print(\"epoch\", \"train loss\", \"validation loss\")\n",
        "  val_loss_curve = []\n",
        "  train_loss_curve = []\n",
        "\n",
        "  # print(epoch)\n",
        "\n",
        "  for ep in range(start_epoch,500):\n",
        "\n",
        "    # Compute train your model on training data\n",
        "    epoch_loss = train(model, train_loader, optimizer, device)\n",
        "\n",
        "    # Validate your on validation data\n",
        "    val_loss = validate(model, val_loader, device)\n",
        "\n",
        "    # Record train and loss performance\n",
        "    train_loss_curve.append(epoch_loss)\n",
        "    val_loss_curve.append(val_loss)\n",
        "\n",
        "    # The learning rate scheduler record the validation loss\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    if (ep + 1) % 20 == 0:\n",
        "      torch.save({\n",
        "          'epoch': epoch,\n",
        "          'model_state_dict':model.state_dict(),\n",
        "          'optimizer_state_dict':optimizer.state_dict(),\n",
        "          'loss':epoch_loss,\n",
        "\n",
        "      },\n",
        "      path + str(ep+1) + \".pt\")\n",
        "\n",
        "    print(ep, epoch_loss, val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1nUlIcwT7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waBnk6mUT7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OGzunPGCT7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTjvWnz0T7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5dDp1B2T7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gq7pYzqT7fc"
      },
      "outputs": [],
      "source": [
        "model_new = PNANet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQVIVBgLT7fc"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load('pretrained_porous_graphene.pt')\n",
        "# model_new.load_state_dict(ckpt['model_state_dict'])\n",
        "optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "\n",
        "#     pred_test = []\n",
        "#     label_test = []\n",
        "#     x_test = []\n",
        "#     for batch in test_loader:\n",
        "#        #node, edge, label, batch = data.x, data.edge_index, data.y, data.batch\n",
        "#        #node = node.to(device)\n",
        "#        #edge = edge.to(device)\n",
        "#        #label = label.to(device)\n",
        "\n",
        "#        # Test the model on each batch\n",
        "#        with torch.no_grad():\n",
        "#          pred = model(batch)\n",
        "#        node = torch.cat([data.x for data in batch]).to(device)\n",
        "#        label = torch.cat([data.y for data in batch]).to(device)\n",
        "#        split_size = [data.x.size()[0] for data in batch]\n",
        "#        pred_split = torch.split(pred, split_size)\n",
        "#        label_split = torch.split(label, split_size)\n",
        "#        x_split = torch.split(node, split_size, dim=0)\n",
        "#        num_graphs = len(pred_split)\n",
        "#        for i in range(num_graphs):\n",
        "#           pred_test.append(pred_split[i].cpu().detach().numpy().squeeze().tolist())\n",
        "#           label_test.append(label_split[i].cpu().detach().numpy().tolist())\n",
        "#           x_test.append(x_split[i].cpu().detach().numpy().tolist())\n",
        "#        torch.cuda.empty_cache()\n",
        "\n",
        "#     write_data(pred_test, label_test, x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ2O0tKWT7fc",
        "outputId": "ddb37d59-c692-4c31-b472-0ce2ea599de7"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Error(s) in loading state_dict for PNANet:\n\tMissing key(s) in state_dict: \"convs.0.aggr_module.avg_deg_lin\", \"convs.0.aggr_module.avg_deg_log\", \"convs.1.aggr_module.avg_deg_lin\", \"convs.1.aggr_module.avg_deg_log\", \"convs.2.aggr_module.avg_deg_lin\", \"convs.2.aggr_module.avg_deg_log\", \"convs.3.aggr_module.avg_deg_lin\", \"convs.3.aggr_module.avg_deg_log\", \"convs.4.aggr_module.avg_deg_lin\", \"convs.4.aggr_module.avg_deg_log\", \"convs.5.aggr_module.avg_deg_lin\", \"convs.5.aggr_module.avg_deg_log\", \"convs.6.aggr_module.avg_deg_lin\", \"convs.6.aggr_module.avg_deg_log\", \"convs.7.aggr_module.avg_deg_lin\", \"convs.7.aggr_module.avg_deg_log\", \"convs.8.aggr_module.avg_deg_lin\", \"convs.8.aggr_module.avg_deg_log\", \"convs.9.aggr_module.avg_deg_lin\", \"convs.9.aggr_module.avg_deg_log\", \"convs.10.aggr_module.avg_deg_lin\", \"convs.10.aggr_module.avg_deg_log\", \"convs.11.aggr_module.avg_deg_lin\", \"convs.11.aggr_module.avg_deg_log\", \"convs.12.aggr_module.avg_deg_lin\", \"convs.12.aggr_module.avg_deg_log\", \"convs.13.aggr_module.avg_deg_lin\", \"convs.13.aggr_module.avg_deg_log\", \"readout.aggr_module.avg_deg_lin\", \"readout.aggr_module.avg_deg_log\". ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_2654015/3379757366.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/anaconda3/envs/IJ_atomicfield/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2041\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2042\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for PNANet:\n\tMissing key(s) in state_dict: \"convs.0.aggr_module.avg_deg_lin\", \"convs.0.aggr_module.avg_deg_log\", \"convs.1.aggr_module.avg_deg_lin\", \"convs.1.aggr_module.avg_deg_log\", \"convs.2.aggr_module.avg_deg_lin\", \"convs.2.aggr_module.avg_deg_log\", \"convs.3.aggr_module.avg_deg_lin\", \"convs.3.aggr_module.avg_deg_log\", \"convs.4.aggr_module.avg_deg_lin\", \"convs.4.aggr_module.avg_deg_log\", \"convs.5.aggr_module.avg_deg_lin\", \"convs.5.aggr_module.avg_deg_log\", \"convs.6.aggr_module.avg_deg_lin\", \"convs.6.aggr_module.avg_deg_log\", \"convs.7.aggr_module.avg_deg_lin\", \"convs.7.aggr_module.avg_deg_log\", \"convs.8.aggr_module.avg_deg_lin\", \"convs.8.aggr_module.avg_deg_log\", \"convs.9.aggr_module.avg_deg_lin\", \"convs.9.aggr_module.avg_deg_log\", \"convs.10.aggr_module.avg_deg_lin\", \"convs.10.aggr_module.avg_deg_log\", \"convs.11.aggr_module.avg_deg_lin\", \"convs.11.aggr_module.avg_deg_log\", \"convs.12.aggr_module.avg_deg_lin\", \"convs.12.aggr_module.avg_deg_log\", \"convs.13.aggr_module.avg_deg_lin\", \"convs.13.aggr_module.avg_deg_log\", \"readout.aggr_module.avg_deg_lin\", \"readout.aggr_module.avg_deg_log\". "
          ]
        }
      ],
      "source": [
        "model_new.load_state_dict(ckpt['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQqCTsaaT7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLpfQMrNT7fc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2j6iYi3T7fc"
      },
      "outputs": [],
      "source": [
        "class PNANet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PNANet, self).__init__()\n",
        "\n",
        "\n",
        "        aggregators = ['mean', 'min', 'max', 'std']\n",
        "        scalers = ['identity', 'amplification', 'attenuation']\n",
        "\n",
        "        self.convs = ModuleList()\n",
        "        self.batch_norms  = ModuleList()\n",
        "        self.grus = ModuleList()\n",
        "\n",
        "        num_layer = args.num_layer\n",
        "        input_dim = args.input_dim\n",
        "        hidden_dim = args.hidden_dim\n",
        "\n",
        "        for i in range(num_layer):\n",
        "            if i == 0:\n",
        "                conv = PNAConv(in_channels=input_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg,\n",
        "                          towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
        "                self.convs.append(conv)\n",
        "                self.grus.append(GRUCell(input_dim, hidden_dim))\n",
        "                self.batch_norms.append(BatchNorm(hidden_dim))\n",
        "            else:\n",
        "                conv = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg,\n",
        "                          towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
        "                self.convs.append(conv)\n",
        "                self.grus.append(GRUCell(hidden_dim, hidden_dim))\n",
        "                self.batch_norms.append(BatchNorm(hidden_dim))\n",
        "\n",
        "        self.readout = PNAConv(in_channels=hidden_dim, out_channels=1, aggregators=aggregators, scalers=scalers, deg=deg,\n",
        "                          towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
        "\n",
        "\n",
        "    def forward(self, data):\n",
        "\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        for conv, gru, batch_norm in zip(self.convs, self.grus, self.batch_norms):\n",
        "            y = conv(x, edge_index)\n",
        "            x = gru(x, y)\n",
        "            x = F.relu(batch_norm(x))\n",
        "        x = self.readout(x, edge_index)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B9EmLEXT7fd",
        "outputId": "32ceaaec-f2b3-43e4-b669-6e2434e4be5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<bound method Module.state_dict of PNANet(\n",
              "  (convs): ModuleList(\n",
              "    (0): PNAConv(2, 50, towers=1, edge_dim=None)\n",
              "    (1-13): 13 x PNAConv(50, 50, towers=5, edge_dim=None)\n",
              "  )\n",
              "  (batch_norms): ModuleList(\n",
              "    (0-13): 14 x BatchNorm(50)\n",
              "  )\n",
              "  (grus): ModuleList(\n",
              "    (0): GRUCell(2, 50)\n",
              "    (1-13): 13 x GRUCell(50, 50)\n",
              "  )\n",
              "  (readout): PNAConv(50, 1, towers=1, edge_dim=None)\n",
              ")>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_new.state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-xVP3A6T7fd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}