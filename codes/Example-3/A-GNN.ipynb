{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIs_zSCibkDN"
   },
   "outputs": [],
   "source": [
    "from os import WIFCONTINUED\n",
    "import numpy as np\n",
    "import os.path as osp\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch import nn\n",
    "from torch_geometric.data import Data, DataLoader, DataListLoader\n",
    "from torch_geometric.utils import degree\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList, Embedding\n",
    "from torch.nn import Sequential, ReLU, Linear, GRUCell\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch_geometric.nn import PNAConv, BatchNorm, global_mean_pool, DataParallel\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyUzgt3ibrlL"
   },
   "outputs": [],
   "source": [
    "# Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-b','--batch_size', default=2, type=int,\n",
    "                    help='batch size')\n",
    "parser.add_argument('-d','--data_path', default='/DATA/graphspiking/data/', type=str,\n",
    "                    help='data path')\n",
    "parser.add_argument('-i','--input_dim', default=2, type=int,\n",
    "                    help='the dimension of coordinates (2D or 3D)')\n",
    "parser.add_argument('-n','--num_data', default=2000, type=int,\n",
    "                    help='the number of all data')\n",
    "parser.add_argument('-l','--num_layer', default=14, type=int,\n",
    "                    help='the number of PNAConv layers')\n",
    "parser.add_argument('-v','--hidden_dim', default=50, type=int,\n",
    "                    help='the hidden dimension of PNANet')\n",
    "parser.add_argument('-m','--max_degree', default=4, type=int,\n",
    "                    help='maximum degree of all nodes')\n",
    "parser.add_argument('-e','--epoch', default=240, type=int,\n",
    "                    help='number of epoch')\n",
    "parser.add_argument('-s','--scale_factor', default=1e-6, type=float,\n",
    "                    help='scale factor for node labels')\n",
    "parser.add_argument('-f')\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.hidden_dim % 5 != 0:\n",
    "    raise Exception(\"Sorry, not available hidden dimension, need to be multiple of 5\")\n",
    "if args.num_layer < 1:\n",
    "    raise Exception(\"Sorry, the number of layer is not enough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XHcqAiMzbrhy"
   },
   "outputs": [],
   "source": [
    "class PNANet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PNANet, self).__init__()\n",
    "\n",
    "\n",
    "        aggregators = ['mean', 'min', 'max', 'std']\n",
    "        scalers = ['identity', 'amplification', 'attenuation']\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.batch_norms  = ModuleList()\n",
    "        self.grus = ModuleList()\n",
    "\n",
    "        num_layer = args.num_layer\n",
    "        input_dim = args.input_dim\n",
    "        hidden_dim = args.hidden_dim\n",
    "\n",
    "        for i in range(num_layer):\n",
    "            if i == 0:\n",
    "                conv = PNAConv(in_channels=input_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                          towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
    "                self.convs.append(conv)\n",
    "                self.grus.append(nn.GRUCell(input_dim, hidden_dim))\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "            else:\n",
    "                conv = PNAConv(in_channels=hidden_dim, out_channels=hidden_dim, aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                          towers=5, pre_layers=1, post_layers=1, divide_input=False)\n",
    "                self.convs.append(conv)\n",
    "                self.grus.append(nn.GRUCell(hidden_dim, hidden_dim))\n",
    "                self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.readout = PNAConv(in_channels=hidden_dim, out_channels=1, aggregators=aggregators, scalers=scalers, deg=deg,\n",
    "                          towers=1, pre_layers=1, post_layers=1, divide_input=False)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        for conv, gru, batch_norm in zip(self.convs, self.grus, self.batch_norms):\n",
    "            y = conv(x, edge_index)\n",
    "            x = gru(x, y)\n",
    "            x = F.relu(batch_norm(x))\n",
    "        x = self.readout(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fztXbgUQbre7"
   },
   "outputs": [],
   "source": [
    "#Train function\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    batch_loss = []\n",
    "    model.train()\n",
    "\n",
    "    for batch in dataloader:\n",
    "        label = torch.cat([data.y for data in batch]).to(device)\n",
    "        # pred = model(batch)       # commenting as code was changed\n",
    "        pred_list=[]\n",
    "        for data in batch:\n",
    "          pred = model(data.to(device))\n",
    "          pred_list.append(pred)\n",
    "\n",
    "        pred_batch = torch.cat(pred_list)\n",
    "\n",
    "        loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
    "\n",
    "        # Backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    return np.mean(np.array(batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZTF5-Ap9brcg"
   },
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, dataloader, device):\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            label = torch.cat([data.y for data in batch]).to(device)\n",
    "\n",
    "            # pred = model(batch)   # changed\n",
    "            pred_list=[]\n",
    "            for data in batch:\n",
    "              pred = model(data.to(device))\n",
    "              pred_list.append(pred)\n",
    "\n",
    "            pred_batch = torch.cat(pred_list)\n",
    "\n",
    "            loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(np.array(val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 770
    },
    "id": "04qt-FUHbrZp",
    "outputId": "3f0c3b92-6507-44ca-f989-32c6af68f1db"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Data preprocessing - can be skipped since data can be directly loaded from dataloader files\n",
    "    \n",
    "    '''\n",
    "    f1 = open(args.data_path + \"edge.txt\", \"r\")\n",
    "    f2 = open(args.data_path + \"node_features.txt\", \"r\")\n",
    "    f3 = open(args.data_path + \"node_labels_sxx.txt\", \"r\")\n",
    "    lines1 = f1.readlines()\n",
    "    lines2 = f2.readlines()\n",
    "    lines3 = f3.readlines()\n",
    "\n",
    "    \n",
    "    num_data = args.num_data\n",
    "    data_list = []\n",
    "    t0 = time.time()\n",
    "    print(\"Number of data processed\\ttime\")\n",
    "    ave = []\n",
    "    for i in range(num_data):\n",
    "        if i % 200 == 0:\n",
    "            print(i, time.time() - t0)\n",
    "        node1 = [int(idx) for idx in lines1[2 * i].split()[1:]]\n",
    "        node2 = [int(idx) for idx in lines1[2 * i + 1].split()[1:]]\n",
    "        edge_index = torch.tensor([node1, node2], dtype=torch.long)\n",
    "        if args.input_dim == 1:\n",
    "          xs = [float(idx) for idx in lines2[i].split()[1:]]\n",
    "          node_feature = [[xs[j]] for j in range(len(xs))]\n",
    "        elif args.input_dim == 2:\n",
    "          xs = [float(idx) for idx in lines2[2 * i].split()[1:]]\n",
    "          ys = [float(idx) for idx in lines2[2 * i + 1].split()[1:]]\n",
    "          node_feature = [[xs[j], ys[j]] for j in range(len(xs))]\n",
    "        elif args.input_dim == 3:\n",
    "          xs = [float(idx) for idx in lines2[3 * i].split()[1:]]\n",
    "          ys = [float(idx) for idx in lines2[3 * i + 1].split()[1:]]\n",
    "          zs = [float(idx) for idx in lines2[3 * i + 2].split()[1:]]\n",
    "          node_feature = [[xs[j], ys[j], zs[j]] for j in range(len(xs))]\n",
    "        elif args.input_dim == 4:\n",
    "          xs = [float(idx) for idx in lines2[4 * i].split()[1:]]\n",
    "          ys = [float(idx) for idx in lines2[4 * i + 1].split()[1:]]\n",
    "          zs = [float(idx) for idx in lines2[4 * i + 2].split()[1:]]\n",
    "          ls = [float(idx) for idx in lines2[4 * i + 3].split()[1:]]\n",
    "          node_feature = [[xs[j], ys[j], zs[j], ls[j]] for j in range(len(xs))]\n",
    "        else:\n",
    "           raise Exception(\"Sorry, not available input dimension\")\n",
    "\n",
    "        x = torch.tensor(node_feature, dtype=torch.float)\n",
    "        node_label = [float(idx) * args.scale_factor for idx in lines3[i].split()[1:]]\n",
    "        y = torch.tensor(node_label, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        # m = data.x\n",
    "        # print(m)\n",
    "        data_list.append(data)\n",
    "\n",
    "    mean_value = np.mean(np.array(ave))\n",
    "    '''\n",
    "\n",
    "    batch_size = args.batch_size\n",
    "    train_loader = DataListLoader(torch.load(\"train_dataset.pt\",weights_only=False))\n",
    "    test_loader = DataListLoader(torch.load(\"test_dataset.pt\",weights_only=False))\n",
    "    val_loader = DataListLoader(torch.load(\"val_dataset.pt\",weights_only=False))\n",
    "\n",
    "    deg = torch.zeros(args.max_degree, dtype=torch.long)\n",
    "    for data in Train_data:\n",
    "        d = degree(data.edge_index[1], num_nodes=data.num_nodes, dtype=torch.long)\n",
    "        deg += torch.bincount(d, minlength=deg.numel())\n",
    "\n",
    "    device = \"cuda:0\"\n",
    "    torch.cuda.empty_cache()\n",
    "    model = PNANet().to(device)\n",
    "    # model = DataParallel(model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=20, min_lr=-1e-5, verbose=True)\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    print(\"Model architecture:\")\n",
    "    print(model)\n",
    "    print(\"The number of trainable parameters is:{}\".format(params))\n",
    "\n",
    "\n",
    "    path = '/DATA/graphspiking/ckpt_orig_2/'\n",
    "    # Training\n",
    "    print(\"epoch\", \"train loss\", \"validation loss\")\n",
    "\n",
    "    val_loss_curve = []\n",
    "    train_loss_curve = []\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "\n",
    "        # Compute train your model on training data\n",
    "        epoch_loss = train(model, train_loader, optimizer,  device=0)\n",
    "\n",
    "        # Validate your on validation data\n",
    "        val_loss = validate(model, val_loader, device=0)\n",
    "\n",
    "\n",
    "        # Record train and loss performance\n",
    "        train_loss_curve.append(epoch_loss)\n",
    "        val_loss_curve.append(val_loss)\n",
    "\n",
    "        # The learning rate scheduler record the validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict':model.state_dict(),\n",
    "                'optimizer_state_dict':optimizer.state_dict(),\n",
    "                'loss':epoch_loss,\n",
    "\n",
    "            },\n",
    "            path + str(epoch+1) + \".pt\")\n",
    "        print(epoch, epoch_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation function\n",
    "def validate(model, dataloader, device):\n",
    "    val_loss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            label = torch.cat([data.y for data in batch]).to(device)\n",
    "\n",
    "            pred_list = []\n",
    "            for data in batch:\n",
    "                pred = model(data.to(device))\n",
    "                pred_list.append(pred)\n",
    "\n",
    "            # Ensure each element in pred_list is a tensor\n",
    "            pred_list = [item if isinstance(item, torch.Tensor) else torch.tensor(item) for sublist in pred_list for item in sublist]\n",
    "\n",
    "            # Concatenate tensors without specifying the dimension\n",
    "            pred_batch = torch.cat(pred_list)\n",
    "\n",
    "            loss = F.mse_loss(pred_batch.squeeze(), label.squeeze())\n",
    "            val_loss.append(loss.item())\n",
    "    return np.mean(np.array(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "checkpoint_path = 'path/to/your/checkpoint.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)  # use device = torch.device('cuda' or 'cpu')\n",
    "\n",
    "# Load the state dict into your model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])  # or just checkpoint if it's a raw state_dict\n",
    "model.to(device)\n",
    "\n",
    "# Calculate test loss using the existing validate function\n",
    "test_loss = validate(model, test_dataloader, device)\n",
    "print(f\"Test Loss: {test_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
